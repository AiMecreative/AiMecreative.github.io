---
title: 【人工智能】人智导论复习
date: 2023-01-05 15:54:09
catecories: ML
---

"复习课的主旨"

"去看GZQ的PPT"

<!--more-->

<font color='red'>然后Jerry告诉了我一些考题, 我的评价是"xx"</font>

# 道听途说的题目归纳

> **什么是人工智能**
> 
> - 答4个定义, 越多越好
>
> **默写BN范式**
>
> ? 有病
>
> **考离散里面的等价公式 (包括他们的名称)**
>
> **计算贝叶斯, 以及贝叶斯网这个图里面元素的意义**

# 绪论: 什么是人工智能, 给出功能模块和实现方法

## 基本概念

> 理性的行动的系统
> 
> - 理性Agent方法: **Agent** 是某种能够行动的东西. **Rational Agent** 可以通过自己的行动获得 **最佳的结果** 或者在不确定的情况下获得 **最佳期望**
> - 功能有 **能做正确的推论**, 若没有能证明正确性的事情, 但是必须 **有所行动**.

> 部分完成理性行动的方法和推论过程无关 (如反射活动)
 
> **图灵测试**
 
> **理性的好处:**
> 
> - 比"思维法则"方法通用, 正确的推论只是实现理性的方法之一
> - 比人类思维更经得起科学的考验 (可以证伪)

## 四种定义

- [ ] 1. 像人一样思考的系统
- [ ] 2. 理性思考的系统
- [ ] 3. 像人一样行动的系统
- [x] 4. 理性行动的系统

## 图灵测试 (Turing Test)

目的是为智能提供一个满足可操作要求的定义

- 如果人类在提出一些书面问题后, 计算回答这些问题, 若无法判断答案是否由人写出, 那么计算机通过了图灵测试 (对于无法给出定义的题目, 应该给出其评价标准)

## 理性 Agent 方法

> **Agent**
> 
> 是某种能够行动的东西, 可以实现自主控制的操作, 感知环境, 持续能力, 适应变化, 以及有能力承担其他Agent的目标
>
> - 要么能做出正确推论, 要么对某件事情有所行动

> **Rational Agent**
> 
> 可以通过自己的行动获得 **最佳的结果** 或者在不确定的情况下获得 **最佳期望的结果**
>
> - 比思维法则通用
> - 比建立在人类行为, 思维基础上的方法更经得起科学发展的检验

> **理性**
>
> - 完美理性: 总能做正确的事情，这在复杂环境下是不可行的。但完美理性是分析的出发点，这样可以简化问题
> - 有限理性: 要求在没有足够计算时间的前提下采取正确的行动


# 智能Agent: Agent的架构。并且要理解Agent的任务、环境、评价标准等

从分析Agent、环境、和它们的关系入手。观察到某些Agent比其它Agent更为出色，从而引出理性Agent的概念：行为表现尽可能好的Agent; 对环境进行分类, 说明环境对Agent的影响; 给出部分Agent的架构或体系结构

## 基本概念

关于智能体, 理性智能体的概念

> **Agent**
> 
> - 通过**传感器**感知所处的环境, 通过**执行器**对环境产生作用
> - 例子 (人类, 机器人, 软件等)
>
> ![agent的工作原理, Agent通过传感器、执行器与环境交互。问号表示从感知到动作的映射函数](agent1.png)

> **Agent和环境**
> 
> - **感知信息**: 任何时刻的感知输入
> - **感知序列**: 所有的输入数据的完整历史
> - Agent在任何时刻的行动选择, 取决于到该时刻为止的整个感知序列
> - Agent函数: 将任意给定感知序列映射到Agent的动作。可以描述Agent的行为。可以用列表表示, 可以通过实验找出感知序列、并记录Agent的行动，来构造该表。Agent函数是抽象的数学表示。Agent程序是具体实现。该程序在Agent自身的结构上运行
> - **理性Agent**是做事正确的智能体, 因此需要一些评判标准, 判断智能体做的正确与否

> **性能度量**
>
> - Agent成功程度的标准
> - 具体而言: 当把Agent置于一个环境中后，它将针对收到的感知信息产生动作序列。该动作序列引起环境历经一个状态序列。如果该状态序列是想要的，则Agent的性能良好。
> - 概括来说就是: **智能体在环境中 > 智能体产生动作序列 > 影响环境 > 环境产生状态序列 > 判断状态序列是否为期望的**

> **理性的判断标准**
>
> - **PPT, 要记**
> - 定义成功标准的性能度量
> - Agent对环境的先验知识
> - Agent可以执行的动作
> - Agent的感知序列

> **理性Agent的标准严谨定义**
> 
> 对于每个可能的感知序列，根据已知的感知序列提供的证据(evidence)和Agent内建的先验知识，理性Agent应该选择期望能**使其性能度量最大化**的行动。
>
> **信息收集是理性的重要部分**: 理性Agent需要经常进行观察
>
> 理性还要求智能体 **从感知的东西中学习**

> **理性和全知**
>
> 全知的智能体, 知道它的动作产生的实际效果, 并且做出相应的动作
>
> **理性不等于完美**: 理性是使期望的性能最大化。而完美是使实际的性能最大化

> **函数计算**: 设计者与Agent是分工和合作的关系
>
> - 设计阶段: 设计者完成一些计算
> - 决策阶段: 思考下一步动作时, Agent会做更多的计算
> - 学习阶段: 从经验中学习的时候, Agent会做更多的计算 **以决定如何修正自己的行为**

> **学习和自主性**
>
> 理性Agent是自主的, 会尽可能学习以弥补不全面, 不正确的先验知识.
>
> 学习是Agent自适应性的基础

关于环境的概念

> **任务环境(PEAS)**
>
> 包括 **性能度量Performance, 环境Environment, 执行器Actuators, 传感器Sensors**
>
> ![PEAS举例](peas-1.png)
>
> 属性集: 完全可观察的、部分可观察的; 确定的、随机的; 情节的、连续的; 静态的、动态的;离散的、连续的; 单Agent、多Agent

## Agent的类型

AI的任务是设计Agent程序, 实现感知信息映射到动作的Agent函数. Agent = 程序 + 体系结构.

> **Agent程序**
>
> 具有相同的框架：从传感器得到当前感知信息作为输入，返回一个动作交给执行器
> 
> ![agent的工作原理, Agent通过传感器、执行器与环境交互。问号表示从感知到动作的映射函数](agent1.png)

> **Agent程序和Agent函数的区别**
>
> 程序: 以当前的感知作为输入
>
> 函数: 以整个感知历史作为输入

> **表驱动的Agent**
>
> Agent程序 > 感知新的信息 > 记录感知序列 > 使用感知序列作为索引，到动作表里查询以决策该做什么 > 动作表明确表示了Agent程序实现的Agent函数。
>
> 存在的问题: 没空间, 没时间, 没指导.

因此可以对Agent进行一些限制, 以满足日常的需求: 不同的Agent区别在于: Agent函数不一样

- **先画图, 再解释**

**简单反射型Agent** : 简单反射型Agent基于**当前的感知**选择行动，忽略其余的感知历史。

可以建造一个通用的 **条件-行动** 规则解释器. (不包含历史信息, 只根据**当天**的环境). 条件-行动规则解释器又称 如果-那么规则, 产生式

![agent-type-1](agent-type-1.png)

> **一些缺点**
>
> 只有在**可以仅根据当前感知信息**来完成决策的情况下才能工作。只有环境完全可观察的情况下，即使有少量不可观测的情况也会引起严重问题
>
> 在部分可观察的环境中，简单的反射型Agent可能陷入无限循环
>
> **随机化**
>
> 随机化可以部分避免无限循环的问题。随机的简单反射Agent其表现可能胜过确定性的简单反射Agent

> **举例**
>
> 人脸识别、语音识别等是简单反射型Agent

**基于模型的反射型Agent**

包含历史信息, 状态信息.

- 内部状态包括: 世界如何独立于Agent而发展的信息; Agent自身的动作如何影响世界的信息

![agent-type-2](agent-type-2.png)

> **举例**
>
> 深度学习中的LSTM、和注意力机制等属于基于模型的反射型Agent

**基于目标的Agent**

不仅需要采用行动, 还需要规定相应的目标. 既要追踪记录世界的状态，又要记录要达到的目标集，并选择能（最终）达到目标的动作

![agent-type-3](agent-type-3.png)

判断执行的动作会不会使得自己离目标更近

> **注意基于目标和基于条件-行动规则有根本的不同**

> **举例**
>
> 路径规划、8数码问题等属于基于目标的Agent

**基于效用的Agent**

单靠目标难以在多数环境中产生更好的行为, 需要更快, 更安全, 更可靠等; 如果一个世界状态比另一个更受偏好，则对Agent来说有更高的效用（utility）

> **效用函数:** 描述Agent与状态相关的偏好程度, 在目标不充分的情况下有助于理性决策
> 
> - 多目标: 加权
> - 存在冲突目标: 折中
> - 拥有显式的效用函数的Agent可以做出理性决策

![agent-type-4](agent-type-4.png)

> **结果的获得**
>
> 最佳期望效用是通过计算所有可能结果状态的加权平均值得到，其权值由结果的概率确定

> **举例**
>
> 机场选址、雄安新区、高铁路线等决策，属于基于效用Agent

**学习Agent**

- 学习元件: 负责改进, 利用来自评价原件的反馈, 评价Agent做得如何, 并决定应该如何修改执行元件以在未来做得更好
- 执行元件: 负责选择外部动作
- 评价元件: 评价Agent做得如何
- 问题产生器: 负责提议可以产生新的, 有启发式价值的经验和动作

![agent-type-5](agent-type-5.png)


# 逻辑Agent

- 命题逻辑的语法
- 推理模式, 方法
- 怪兽世界的推理 ($R_1-R_{10}$的证明, $R_{11}-R_{15}$将句子转化成合取范式, 然后基于归结或反证法的证明)
- 理解推理规则和公理, 用于命题逻辑的推理
- 命题逻辑的连接符及语义

## 命题逻辑语法

![logic-1](logic-1.png)

## 推理模式, 推理方法

- 画真值表 (不推荐, 繁琐)
- 运用公式法则 (重要, 要记)

![logic-2](logic-2.png)

![logic-3](logic-3.png)

- 归结 :star: 只能取一项进行归结, 不能同时取很多项, 只用于文字的析取式
- 合取范式: 文字析取式的合取形式, CNF, **如何转化**

# 知识表示

- 情景演算, 情景, 流, 前提条件公理 (可能性公理), 效应公理等概念 (背)
- 本体论, 类别, 对象, 举例说明 (背)

讨论的是 **如何表示世界的事实**, 着重于在不同领域都会出现的通用概念, 如动作, 时间, 物理对象, 信念, 表示这样的抽象概念是 **本体论工程(en?)**

## 本体论工程

以高某PPT为准

## 类别和对象

以高某PPT为准

## 动作, 情景, 事件, 流

对动作结果的推理是基于知识的Agent运作的中心问题

### 情景演算本体论

![情景演算](情景演算.png)

**流 fluent**

- 使情景从一个变换到下一个的函数和谓词, 如Agent的位置或怪兽的死活

在定义谓词或函数时, 允许不受时间影响

# 概率推理

- 贝叶斯网络的定义, 语义
- 给定贝叶斯网络, 写出相应的联合概率分布
- 针对书中的盗贼警报任务, 理解贝叶斯网络的精确推理, 并能够在给定证据的情况下进行推理

## 贝叶斯网络的定义和语义

独立性, 条件独立性, 不确定域中的知识表示

- 减少了所需定义的概率数目

有向图, 结点代表概率信息

- 随机变量组成的网络结点
- 有向边集合
- 每个结点有条件概率 $P(X_i | Parents(X_i))$
- 无环

# 指定简单决策

- 理解什么是偏好, 效用, 最大期望效用原则 (计算期望)
- 理解决策网络, 尤其是网络中的结点类型
- 给定网络, 计算不同动作的期望效用, 并选具有最大期望效用的动作

## 在不确定性环境下结合信度和愿望

![expectation-1](expectation-1.png)

