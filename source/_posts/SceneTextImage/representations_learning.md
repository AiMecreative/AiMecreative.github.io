---
title: Paper | Represention Learning in Scene Text Images
author: Mecreative
date: 2024-12-03 15:32:13
category: Scene Text Image
tag: Scene Text Image
---


## Multi-modal

---

**[CVPR24] ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting** [(Paper)](https://openaccess.thecvf.com/content/CVPR2024/html/Duan_ODM_A_Text-Image_Further_Alignment_Pre-training_Approach_for_Scene_Text_CVPR_2024_paper.html) [(Code)](https://github.com/PriNing/ODM)

本文主要是实现了文本图像的去形式化，也就是将图片中的文本转化为标准字体，并保持一定的旋转和位置不变，输出在图片上，是一种预训练方式。但是该方式还是使用了标签作为监督。

<center>**---------- Abstracts ----------**</center>

- **Motivation**
  - in Optical Character Recognition (OCR) tasks, <u>aligning text instances with their corresponding text regions in images poses a challenge</u>, as it requires effective alignment between text (text annotation) and OCR-Text (text in image) rather than a holistic understanding of the overall image content
  - transfers diverse styles of text found in images to a uniform style based on the text prompts
  - costs in scene text image annotations
- **Methods**
  - ODM model, transfer diverse form into plain form
  - label generation, to allow large amount of unlabeled data to participate in pretraining

<center>**---------- Related Work ----------**</center>
<center>**---------- Methods ----------**</center>

This is a framework for pretraining, so the encoder and decoder can be any model that is suitable for spotting and detection tasks. The connection between image encoder and text encoder is a cross attention mechanism. The delicately designed part is losses.

- binary segementation loss: for every pixel, calculate binary corss-entropy loss $\mathcal{L}_{1}=\frac{1}{N} \sum_{i=1}^N - [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)]$
- OCR-LPIPS loss: aims to constrain the features, the output binary and ground truth images are inputed to a well trained detector (e.g. UNet-VGG with $l$ layers), and then calculate the sum of layer loss $\mathcal{L}_2=\sum_{l} \frac{1}{H_l W_l} \| VGG_l(\widehat{y}) - VGG_l(y) \|_1$
- contrastive loss: maps texts and images into the same semantic space, calculate loss $\mathcal{L}_3 = CE(\frac{\exp(I, T_+)}{\sum_{i=1}^B \exp(I, T_i)}, y_I) + CE(\frac{\exp(T, I_+)}{\sum_{i=1}^B \exp(T, I_i)}, y_T)$
- total loss: $\mathcal{L} = \mathcal{L}_1 + \mathcal{L}_2 + 0.5 \mathcal{L}_3$

---

**[CVPR24] Multi-modal In-Context Learning Makes an Ego-evolving Scene Text Recognizer** [(Paper)](https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Multi-modal_In-Context_Learning_Makes_an_Ego-evolving_Scene_Text_Recognizer_CVPR_2024_paper.html) [(Code)](https://github.com/bytedance/E2STR)

<center>**---------- Abstracts ----------**</center>

- **Motivations**
  - difficult to transfer to different domain for STR (such as font sdiversity, shape deformations)
  - fine-tuning is computationally intensive and requires multiple model copies for various scenarios
  - LLMs with in-context learning fail as the insufficient incorporation of contextual information form diverse samples in the training stage (?)
  - In summary: STR models needs fine-tuning to satisfy new scenorios which costs too much. So this paper wants to borrow the ICL in LLM to decrease the costs 
- **Methods**
  - train with <u>context-rich</u> scene text sequences (sequences are generated by in-context training strategy)
  - regular size model is enough
- **Questions**
  - what is "context-rich", how to depict "rich"

<center>**---------- Related Work ----------**</center>

- Multi-modal in In-context learning, training-free
  - LLMs can quickly adapt to new tasks with just a few examples (treat these inputs as prompt), this phenomenon is a new learning paradigm termed "In-Context Learning", which means "The label is the input itself". But it's difficult to transfer the learning paradigm to VLMs

<center>**---------- Methods ----------**</center>

**Model Architecture**

![E2STR Model](image-1.png)

1. Model trained in the standard auto-regressive paradigm to learn fundamental STR ability
2. In-Context training, learn to understand the connection among different samples
3. inference, fetches in-context prompts based on visual similarity

**Training Strategy**

1. Train with original training set
2. Generate splited and transformed samples and then concatenate them in a sequence form. Train with these sequences.

**Inference**

Inference needs to maintain an In-Context Pool, where the k-NN seletion strategy will be conducted. k-NN will select top-K similar samples in latent space to form the prompts

![E2STR split strategy](image-2.png)

> **Attention**
>
> - The split phase faces the difficulty of alignment, especially for art texts, which can't be splited by a single *rectangle* (so what about a deformable shape?)
> - Inference should maintain an In-Context pool

---

## Distangle Representions

---

**[CVPR24] Choose What You Need: Disentangled Representation Learning for Scene Text Recognition Removal and Editing**
[(Paper)](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Choose_What_You_Need_Disentangled_Representation_Learning_for_Scene_Text_CVPR_2024_paper.html)

这篇文章主要是使用了额外的数据集用于提取解耦特征，从数据角度解决特征耦合的问题。

<center>**---------- Abstracts ----------**</center>

![arch_1](image.png)

- **Motivations**
  - previous representation learning methods use tightly coupled features for all tasks, resulting in sub-optimal performance
  - disentangling these two types (style feature and context feature) of features for improved adaptability in better addressing various downstream tasks
- **Methods**
  - <u>Dataset</u>: we synthesize a dataset of image pairs with identical style but different content, the dataset generator is [SynthTIGER](https://github.com/clovaai/synthtiger)
  - <u>Losses</u>: content features are supervised by a text recognition loss, while an alignment loss aligns the style features in the image pairs

<center>**---------- Related Work ----------**</center>

To illustrate the drawbacks of using tightly coupled features.

- STR with segementation preprocessing
  - [SIGA](https://openaccess.thecvf.com/content/CVPR2023/papers/Guan_Self-Supervised_Implicit_Glyph_Attention_for_Text_Recognition_CVPR_2023_paper.pdf)
  - [CCD](https://openaccess.thecvf.com/content/ICCV2023/papers/Guan_Self-Supervised_Character-to-Character_Distillation_for_Text_Recognition_ICCV_2023_paper.pdf)
  - [Deformable CNN](https://zhuanlan.zhihu.com/p/138020203) use a learnable bias in convolution block to represent deformations

<center>**---------- Methods ----------**</center>

The total model is divided into two part, one for generation and the other for recognition. Both parts use MSA to extract features but with different losses. The gradients in context part are blocked to realize decouple target. For multiply layer, model uses gated strategy to fuse multi-layer features. 

> **Attention**
> 
> - This model divides style and context fetures with the SAME lengths, which may constrain the ability to represent background information, because the background is assumed more complex than texts.

---

