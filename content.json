{"meta":{"title":"Mecreative","subtitle":"醉后不知天在水，满船清梦压星河","description":"","author":null,"url":"https://aimecreative.github.io","root":"/"},"pages":[{"title":"二十岁的自传","date":"2024-12-29T06:44:40.006Z","updated":"2024-12-29T06:44:40.006Z","comments":true,"path":"about/index.html","permalink":"https://aimecreative.github.io/about/index.html","excerpt":"","text":"我在十九岁最后两天的时候给自己写了点东西，当时在教室，周围是考研的学长学姐。 现在是二十岁的我，自传嘛，随便写写，但是并不代表对自己的亵渎。 写点什么呢。 弱冠年，本科在读。 二零年毕业于郴州市一中。 高考延期，现在仍然记得高考时的座位靠窗，那天很热。 疫情在高三开始， 高二的回忆是关于粉橙色的夕阳和理综数学， 高一开始当了纪律委员， 夏令营的天很蓝。 初三的风，初二的她，初一的混乱和美术。 县城小学四年，与父骑车游玩， 浑身是泥，不汗不归。 一二年级在积木中度过。 宅居校内，父母为师， 门前青草针叶衫，夜晚是母亲扇风的手。 再往前，记忆只在照片中凝固。 就这样，平常地活着，安然无恙地活着。 当时怎知宇宙之大，也从不担忧人生几何； 现在知道了宇宙的度量，明白了人生几何， 目睹了活着，和死亡，目睹了一个时代的结束。 人体维护的一切，只是将熵增的速率变缓。 但那又怎样。 我欣然接受。 我已经学会了走路，奔跑；说话，呐喊；回忆，思考；见面，告别。 并且我仍然会 奔跑着，呐喊着，思考着，以及告别着。 但无论怎样，请别忘了： 走路，说话，回忆和见面， 也是你的本能。"},{"title":"categories","date":"2024-12-29T06:44:40.006Z","updated":"2024-12-29T06:44:40.006Z","comments":true,"path":"categories/index.html","permalink":"https://aimecreative.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2024-12-29T06:44:40.007Z","updated":"2024-12-29T06:44:40.007Z","comments":true,"path":"css/prism.css","permalink":"https://aimecreative.github.io/css/prism.css","excerpt":"","text":"/** * prism.js default theme for JavaScript, CSS and HTML * Based on dabblet (http://dabblet.com) * @author Lea Verou */ code[class*=\"language-\"], pre[class*=\"language-\"] { color: black; background: none; text-shadow: 0 1px white; font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; word-wrap: normal; line-height: 1.5; -moz-tab-size: 4; -o-tab-size: 4; tab-size: 4; -webkit-hyphens: none; -moz-hyphens: none; -ms-hyphens: none; hyphens: none; } pre[class*=\"language-\"]::-moz-selection, pre[class*=\"language-\"] ::-moz-selection, code[class*=\"language-\"]::-moz-selection, code[class*=\"language-\"] ::-moz-selection { text-shadow: none; background: #b3d4fc; } pre[class*=\"language-\"]::selection, pre[class*=\"language-\"] ::selection, code[class*=\"language-\"]::selection, code[class*=\"language-\"] ::selection { text-shadow: none; background: #b3d4fc; } @media print { code[class*=\"language-\"], pre[class*=\"language-\"] { text-shadow: none; } } /* Code blocks */ pre[class*=\"language-\"] { padding: 1em; margin: .5em 0; overflow: auto; } :not(pre) > code[class*=\"language-\"], pre[class*=\"language-\"] { background: #f5f2f0; } /* Inline code */ :not(pre) > code[class*=\"language-\"] { padding: .1em; border-radius: .3em; white-space: normal; } .token.comment, .token.prolog, .token.doctype, .token.cdata { color: slategray; } .token.punctuation { color: #999; } .token.namespace { opacity: .7; } .token.property, .token.tag, .token.boolean, .token.number, .token.constant, .token.symbol, .token.deleted { color: #905; } .token.selector, .token.attr-name, .token.string, .token.char, .token.builtin, .token.inserted { color: #690; } .token.operator, .token.entity, .token.url, .language-css .token.string, .style .token.string { color: #9a6e3a; /* This background color was intended by the author of this theme. */ background: hsla(0, 0%, 100%, .5); } .token.atrule, .token.attr-value, .token.keyword { color: #07a; } .token.function, .token.class-name { color: #DD4A68; } .token.regex, .token.important, .token.variable { color: #e90; } .token.important, .token.bold { font-weight: bold; } .token.italic { font-style: italic; } .token.entity { cursor: help; }"},{"title":"tags","date":"2024-12-29T06:44:40.010Z","updated":"2024-12-29T06:44:40.010Z","comments":true,"path":"tags/index.html","permalink":"https://aimecreative.github.io/tags/index.html","excerpt":"","text":""},{"title":"","date":"2024-12-29T06:44:40.007Z","updated":"2024-12-29T06:44:40.007Z","comments":true,"path":"css/site.css","permalink":"https://aimecreative.github.io/css/site.css","excerpt":"","text":"p code, li code, h1 code, h2 code, h3 code { display: inline-block; white-space: no-wrap; background: #f6f6f6; font-size: .9em; line-height: 1.5em; color: #464646; border: 1px solid #eeeeee; -webkit-border-radius: 0.4em; -moz-border-radius: 0.4em; -ms-border-radius: 0.4em; -o-border-radius: 0.4em; border-radius: 0.4em; padding: 0 .3em; margin: -1px 0; } .article-content a { color: #21afd3; } .article-content a:hover { color: #ff6557; text-decoration: underline; } @font-face { font-family: 'CaskaydiaCove Nerd Font'; src: url('/fonts/subset-CaskaydiaCoveNerdFontComplete-.eot'); src: url('/fonts/subset-CaskaydiaCoveNerdFontComplete-.eot?#iefix') format('embedded-opentype'), url('/fonts/subset-CaskaydiaCoveNerdFontComplete-.woff2') format('woff2'), url('/fonts/subset-CaskaydiaCoveNerdFontComplete-.woff') format('woff'), url('/fonts/subset-CaskaydiaCoveNerdFontComplete-.ttf') format('truetype'), url('/fonts/subset-CaskaydiaCoveNerdFontComplete-.svg#CaskaydiaCoveNerdFontComplete-') format('svg'); font-weight: normal; font-style: normal; font-display: swap; } code[class*=\"language-\"], pre[class*=\"language-\"] { border-radius: .6em; font-family: 'CaskaydiaCove Nerd Font' !important; } code { font-family: 'CaskaydiaCove Nerd Font' !important; } .article-content { font-family: 'CaskaydiaCove Nerd Font'; } .original { margin: 2em 0 0; padding: .5em 1em; border-left: 3px solid #fbbc97; background-color: #f9f9f9; font-size: 14px; list-style: none; } .footnote-item p { line-height: 0; } .article-content li { line-height: 2em; }"}],"posts":[{"title":"Lecture | Database Review","slug":"Lecture/database_review","date":"2024-12-29T06:58:17.000Z","updated":"2024-12-30T09:19:26.595Z","comments":true,"path":"2024/12/29/Lecture/database_review/","link":"","permalink":"https://aimecreative.github.io/2024/12/29/Lecture/database_review/","excerpt":"","text":"Data什么是数据数据的集合是数据库的主要研究对象之一。数据集通常具有大规模、有结构等特征。因此为了方便研究，下面先探究数据的定义和表示方式。 首先数据是一种数学对象，而数据的集合自然在数学概念上拥有集合之间的各种运算。例如集合之间的关系包括交并补差。集合可以赋予一定的代数结构，使得数据之间可以进行运算。在集合的角度下还可以为数据赋予拓扑概念，由此可以考察数据间是否存在邻近关系。除此之外，若对集合赋予距离，我们还可以度量数据之间的距离和相似度。若数据之间可以定义序关系，则可以在数据集上实现各种各样的算法，例如排序。 Relation 关系当把数据视作是关系时，可以从结构和运算角度进行分析。 偏序关系：当数据中可以定义序结构时，数据之间的关系存在偏序关系，因此可以在上面定义一系列的算法。最典型的例子是我们可以根据一定的偏序关系对数据进行排序 拓扑关系：如果需要考察数据之间的相邻结构，则可以从拓扑关系上考察数据的结构 运算关系：可以在数据之间定义运算，考察数据之间的代数结构 关系直接是可以进行运算的。由于关系是集合，因此必然可以进行集合间的运算。常见的关系运算包括笛卡尔积、投影运算、选择运算、交并补差和判断元素是否属于集合等运算。而通过关系间的运算，我们又可以产生出新的集合。 度量关系：如果可以在数据上定义某种度量，进而定义出范数和距离，可以考察数据上的测度结构 当数据具有一定的结构时，非常适合用关系模型描述数据。 Geometry 几何这是一个比较有趣的角度。将数据视作高维空间中的点，而事实上，在AI领域，几乎所有工作都是将实体数据编码成高维空间的中的向量。对于一般的数据而言，我们同样可以将其视作空间中的一点，数据之间的关系转化为空间中点之间的关系，比较特殊的关系包含点的共线、共面问题。而具有一定关系的点的集合可能可以在空间中具有某种特殊的几何结构，或与某个几何结构有着同构的关系。更一般而言，所有的数据分布在高维空间中的流形上，我们使用的数据是在流形上的采样。而实际上在某些任务中真正有用的数据并不需要非常高维的流形采样，数据带有一定的冗余，在这些任务中只需要一些低维的投影就能表示这些数据并且符合任务需要。如此一来就必然存在一个映射，使得高维流形上的数据能与低维空间中的某些点一一对应。由于映射是一一的，因而同样存在反函数使得这些低维空间中的点能映射回高维空间。而事实上，我们只知道这些函数是一定存在的，但并不能获得显式的表达式，因此称为隐函数。 流形假设：现实中很多高维数据集实际上是沿着高维空间内的低维潜流形分布，即潜特征嵌入空间。流形假设的结果是看起来需要很多变量描述的数据集，实际上只需要较少变量就能完成描述。在高维流形的局部，可以用低维的欧几里得空间进行近似，而欧几里得空间可以很容易找到对应的基底来描述该流形的局部。 在数据不具有一定的结构时，比较适合用流形结构描述数据，如从现实数据中采样的文本、图像和视频数据，都可以假设其分布在一定的流形结构上，进而得知存在一个隐函数可以将这些数据映射到低维空间中。映射完成后可以在低维空间中分析数据的结构并进行数据查询，这也是目前大部分深度学习模型需要学习的模式。 Internet 网络互联网的特点包括开放、大数据、去中心化等，当数据在网络上进行共享时，其传输过程可以用图来表示。问题在于互联网的开放性和数据的存储仿佛是相悖的。互联网上的数据允许任何人进行修改，而数据的存储需要保持一致性。因此为了让互联网实现数据的存储并保持数据的一致性，互联网上的数据必须遵守数据共识（Agreement），即数据的值是互联网群体所认同的值，互联网的各个使用者对该值具有共识，而某一方强制修改该值时，此时被修改的数据只能获得小部分的认同，因此修改后的值不被互联网所认可，从而保持了数据存储的一致性。 由于互联网的规模很大，因此要求系统具有很强的可扩展性。可扩展性分为存储扩展性和计算扩展性，从而衍生出来了P2P模型和Map-Reduce操作 Artificial Intelligence 人工智能数据与人工智能有很强的联系。 Reasoning：推理实际上是数据集合上的运算。其中推理中涉及到的项和函数与集合中的元素对应，而推理中的谓词与集合或者关系对应，逻辑演算即集合的运算。在此基础上，一阶逻辑演算可以简化为关系代数运算。 Learning：通过学习得到的模型能在一定程度上反映对象的结构（一般而言是非结构化对象），且数据是高维空间中的点，而学习方式可以实现数据维度之间的隐函数依赖关系。因而学习通过几何模型，可以抓取非结构对象内部的函数依赖关系。 Data Model结构化数据和非结构化数据是可收集数据的两大类别，结构化数据是整齐地放入数据表的数据，包括数字、短文本和日期等离散数据类型。由于其大小或性质，非结构化数据无法整齐地放入数据表：例如，音频和视频文件以及大型文本文档。更多资料。 数据模型要求具有可表达的、有效的等特点。因此部分数据模型需要一定的元数据来描述数据结构。数据结构即一定的schema模式。 结构化数据数据模型是用来描述结构化数据的概念和工具，一般维护了数据的结构、运算和限制。常用的数据模型有关系型数据模型和图数据模型等。 关系型数据模型：关系型数据模型中所有的数据都被表示成表格，表格抬头反映的是该组数据遵循的模式。表格的标题定义的是该组数据的一组属性，每个属性有属性名和数据类型。而表格的主体部分则用于存储实际的数据，一条数据对应表格主体中的一行，一条数据中的每个值都唯一对应标题中的属性。该数据模型上的运算本质上还是集合间的运算。 在该数据模型上的查询：用户对该数据模型的一个查询对应于求集合间运算结果的一个子集。不同的数据表格通过笛卡尔积连接在一起，然后通过某些运算后，最后再进行选择操作，得到查询结果。 非结构化数据非结构化的数据可以看作是数据空间中的一个点。 数据空间：数据空间可以表示成定义了一定结构的数据集合（域）。 非结构化的数据采样自数据空间。设数据空间 $D = \\lbrace (xi, y_i) \\rbrace{i=0}^n，采样集合S \\sub D。为了让S更加能反映D的分布特性，通常要求S是D中的均匀采样。要描述这样的非结构化数据往往需要训练一个在S上的模型f_w^S，使得对于输入的x_i所计算得到的\\hat{y_i}与y_i之间的误差能最小，其中(x_i, y_i) \\in S，此时给定一个采样S，我们就能训练出对应的模型f_w^S，即模型f_w^S可以看作是对数据集S的一个编码或者表示。我们期望训练出来的模型效果对于不在S中的数据(x_i, y_i) \\in D \\ S$ 同样有着很好的拟合能力。 互联网数据互联网数据通常是开放的和大规模的。由于其开放性，任何类型的数据都可以存储在网上，因此需要的schema-free的数据模型。而由于其大规模，因此数据的查询需要支持大规模并发和张量计算，同时查询的数据需要去冗余，即需要去中心化。 人工智能在AI中，推理与数据模型中的逻辑数据相对应，逻辑数据可以通过一阶逻辑谓词进行演算，即数据之间的关系代数演算。而AI中归纳学习可以对应于非结构化的数据，AI学习的模型是对数据集合的一种编码，模型可以描述数据集合的某些性质。 Data System数据的扩展性讨论计算机体系结构数据对应的计算模型与当前的计算机体系结构有关，经典的体系结构为Von Neumann模型。这类型的计算机在进行运算时，CPU会从多级的存储结构中读取/写入数据，而其中的瓶颈就在于读写磁盘的速度。目前较为前沿的技术聚焦于存算一体化、超算模型等。当前缓解瓶颈的读写速度体现在分布式并行、云计算、P2P计算等技术。所有的数据在分布式系统中的进行传输构成数据流，而对于张量形式的数据，即为张量流。 存储扩展性问题建模：我们希望数据的存储能最小化IO次数。假设 是schema 的实例， 是磁盘块 $Bi的集合Disk = \\lbrace B_i \\rbrace{i=0}^n。设映射m: r(R) \\rightarrow Disk是r(R)存储的数据结构。用户的查询Q是r(R)的子集Q \\sub r(R)$。我们的目标是求解 \\argmin_m \\| m(Q) \\|也就是需要设计一种索引方式，使得每一个查询来时能快速访问到对应的数据。目前常用Hash形式或树形式进行数据的索引结构。 对于单结点单磁盘块的存储、对于单结点多磁盘块的存储、对于多结点的存储。 计算扩展性由于目前的数据量愈发庞大，因此要求一些计算模型能够支持大规模同步并发（BSP）。在张量层可以经过TPU进行并行计算，而在计算图层分析和解决图之间的依赖问题。为了实现BSP，现代的计算模型实现了Map-Reduce操作，通过编写Map-Reduce并将这些操作分配给对应的线程，可以实现同步并发。 哪些集合运算可以并发执行 如何在同步并发时保持数据的一致性 调度和序列化，加锁协议 Transaction 数据的高效性讨论吞吐量吞吐量指的是单位时间内处理的transaction数量，由系统的并行能力和调度器共同决定 处理单位查询所需时间由系统的IO索引机制优化决定 Application","categories":[{"name":"Lecture","slug":"Lecture","permalink":"https://aimecreative.github.io/categories/Lecture/"}],"tags":[{"name":"Lecture","slug":"Lecture","permalink":"https://aimecreative.github.io/tags/Lecture/"},{"name":"Database","slug":"Database","permalink":"https://aimecreative.github.io/tags/Database/"}],"author":"Andrew-Rey"},{"title":"Paper | Represention Learning in Scene Text Images","slug":"SceneTextImage/representations_learning","date":"2024-12-03T07:32:13.000Z","updated":"2024-12-29T06:20:50.401Z","comments":true,"path":"2024/12/03/SceneTextImage/representations_learning/","link":"","permalink":"https://aimecreative.github.io/2024/12/03/SceneTextImage/representations_learning/","excerpt":"","text":"Vision Transformer RoFormer: Enhanced Transformer with Rotary Position Embedding (Paper) (Code) (Ref) (Note&amp;Blog) [CVPR22] Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers (Paper) (Code) (Issue) [ICLR25] An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels (Paper) Meta的一篇文章，但还在ICLR的rebuttle。主要观点是用单独的pixel作为token可以将local的归纳偏置降为零。文章类比bag of words，提出set of pixels，并在其上加上必要的positional embeddings后，输入普通的Transformer可以学习特征可以比切割patch的方式表现更好。当然计算量也更大。重要的是，作者提出可以选择性地输入像素，例如上面的一篇文章。由于没有具体的新模型，只是一个普通的研究，这里记录一些自己的笔记。 ViT的输入不再是以patch的方式，也就是认为patch和使用连续函数进行位置编码都会来带一定程度的归纳偏置。因此改为pixel的方式，并使用可学习的位置编码可以消除由于locality带来的归纳偏置。 将输入改为pixel后，输入的方式就变得比较多样。可以是随机的像素输入，也可以是有结构性地输入，还可以是输入重要的像素等等。文章在提到这些问题的时候引用了这篇文章。 可学习的位置编码很重要。 无监督学习可以快速学到像素之间的想似性和像素之间的结构特征。 如果有标注，依然存在对齐的问题。但是否存在一种方式可以先通过像素为本的预训练学习结构特征，再通过微调快速对其多模态特征。 例如text images 重要的不是学习像素（因为只有255类，相当于26字母），而是学习像素之间的相对关系，只有满足某种关系的像素集合，才能构成图像。对于text images而言，这种关系需要满足： 这种关系应该是像素集合（）到图像（）之间的映射，即一个像素需要到一个二维坐标，每个坐标需要有一个像素值，但每个像素值不一定需要坐标，即定义域是像素集合的子集 能将像素映射到正确的位置从而呈现出人眼可识别的图像 被映射到图像中文本区域的像素需要和embedding后的文本特征对应 更强的条件：被映射到文本区域的像素需要按照顺序与embedding后的文本对齐 继续思考：embedding后的文本实质上对应的是图像上的一块区域 是否可以把文本进行二维编码 因此需要模型能在预训练时得到图像中各个区域的隐变量表示，而在微调时快速建立其它模态与这些隐变量之间关系 [ECCV22] CoMER: Modeling Coverage for Transformer-based Handwritten Mathematical Expression Recognition (Paper) (Code) 本文主要针对手写数学公式识别任务魔改了ViT，把Coverage Attention引入ViT的特征层，优化Attention Weights，并进行并行解码，在降低过往注意过的区域的同时提高了解码效率。 ---------- Abstracts ---------- Background Coverage Attention: handle the problem in translation i.e. over-translation and under-translation, mainly used in RNN, where Coverage Attention can use the past hidden states and forces the model to attend more on unparsed parts in image. Derictly introducing Coverage Attention to ViT will break the parallel decoding of Transformer. Motivation introduces Coverage Attention into Transformer's attention layer to refine the attention weights in a prefix-sum form without hurting parallel decoding Methods designs three types of attention refinement methods i.e. self-, cross- and fusion-coverage ---------- Related Work ---------- Coverage Attention in RNN: for attention map on features , the coverage vector can be calculated by (the prefix sum of column vectors of attention map). Then the coverage matrix is . For RNN, there are hidden states at time step , they constract the hidden state matrix and learnable weights , the attention vector can be get as follow: ---------- Methods ---------- Multipling and firstly and then add it to which can decreases the space complexity: So the attention item arm The codes can be found in github Attention In fact, this paper realized an alignment methods: when predicting the label at step , the refined attention weights attend the next region in image, the region in image aligns the label character at Multi-modal [CVPR24] ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting (Paper) (Code) 本文主要是实现了文本图像的去形式化，也就是将图片中的文本转化为标准字体，并保持一定的旋转和位置不变，输出在图片上，是一种预训练方式。但是该方式还是使用了标签作为监督。 ---------- Abstracts ---------- Motivation in Optical Character Recognition (OCR) tasks, aligning text instances with their corresponding text regions in images poses a challenge, as it requires effective alignment between text (text annotation) and OCR-Text (text in image) rather than a holistic understanding of the overall image content transfers diverse styles of text found in images to a uniform style based on the text prompts costs in scene text image annotations Methods ODM model, transfer diverse form into plain form label generation, to allow large amount of unlabeled data to participate in pretraining ---------- Methods ---------- This is a framework for pre-training, so the encoder and decoder can be any model that is suitable for spotting and detection tasks. The connection between image encoder and text encoder is a cross attention mechanism. The delicately designed part is losses. binary segementation loss: for every pixel, calculate binary corss-entropy loss OCR-LPIPS loss: aims to constrain the features, the output binary and ground truth images are inputed to a well trained detector (e.g. UNet-VGG with layers), and then calculate the sum of layer loss contrastive loss: maps texts and images into the same semantic space, calculate loss total loss: [CVPR24] Multi-modal In-Context Learning Makes an Ego-evolving Scene Text Recognizer (Paper) (Code) ---------- Abstracts ---------- Motivations difficult to transfer to different domain for STR (such as font sdiversity, shape deformations) fine-tuning is computationally intensive and requires multiple model copies for various scenarios LLMs with in-context learning fail as the insufficient incorporation of contextual information form diverse samples in the training stage (?) In summary: STR models needs fine-tuning to satisfy new scenorios which costs too much. So this paper wants to borrow the ICL in LLM to decrease the costs Methods train with context-rich scene text sequences (sequences are generated by in-context training strategy) regular size model is enough Questions what is \"context-rich\", how to depict \"rich\" ---------- Related Work ---------- Multi-modal in In-context learning, training-free LLMs can quickly adapt to new tasks with just a few examples (treat these inputs as prompt), this phenomenon is a new learning paradigm termed \"In-Context Learning\", which means \"The label is the input itself\". But it's difficult to transfer the learning paradigm to VLMs ---------- Methods ---------- Model Architecture E2STR Model Model trained in the standard auto-regressive paradigm to learn fundamental STR ability In-Context training, learn to understand the connection among different samples inference, fetches in-context prompts based on visual similarity Training Strategy Train with original training set Generate splited and transformed samples and then concatenate them in a sequence form. Train with these sequences. Inference Inference needs to maintain an In-Context Pool, where the k-NN seletion strategy will be conducted. k-NN will select top-K similar samples in latent space to form the prompts E2STR split strategy Attention The split phase faces the difficulty of alignment, especially for art texts, which can't be splited by a single rectangle (so what about a deformable shape?) Inference should maintain an In-Context pool Distangle Representions [CVPR24] Choose What You Need: Disentangled Representation Learning for Scene Text Recognition Removal and Editing (Paper) 这篇文章主要是使用了额外的数据集用于提取解耦特征，从数据角度解决特征耦合的问题。 ---------- Abstracts ---------- arch_1 Motivations previous representation learning methods use tightly coupled features for all tasks, resulting in sub-optimal performance disentangling these two types (style feature and context feature) of features for improved adaptability in better addressing various downstream tasks Methods Dataset: we synthesize a dataset of image pairs with identical style but different content, the dataset generator is SynthTIGER Losses: content features are supervised by a text recognition loss, while an alignment loss aligns the style features in the image pairs ---------- Related Work ---------- To illustrate the drawbacks of using tightly coupled features. STR with segementation preprocessing SIGA CCD Deformable CNN use a learnable bias in convolution block to represent deformations ---------- Methods ---------- The total model is divided into two part, one for generation and the other for recognition. Both parts use MSA to extract features but with different losses. The gradients in context part are blocked to realize decouple target. For multiply layer, model uses gated strategy to fuse multi-layer features. Attention This model divides style and context fetures with the SAME lengths, which may constrain the ability to represent background information, because the background is assumed more complex than texts.","categories":[{"name":"Multi-Modal","slug":"Multi-Modal","permalink":"https://aimecreative.github.io/categories/Multi-Modal/"}],"tags":[{"name":"Multi-Modal","slug":"Multi-Modal","permalink":"https://aimecreative.github.io/tags/Multi-Modal/"}],"author":"Mecreative"},{"title":"Lecture | Algorithm Review","slug":"Lecture/algorithm_review","date":"2024-12-03T07:32:13.000Z","updated":"2024-12-29T06:58:56.888Z","comments":true,"path":"2024/12/03/Lecture/algorithm_review/","link":"","permalink":"https://aimecreative.github.io/2024/12/03/Lecture/algorithm_review/","excerpt":"","text":"Algrithm Review Notes Basic 简述快速排序中划分过程的作用。 快速排序划分可以将整个数组划分为两部分，使得数组右边小于左边（升序），而用于划分数组的基准元被放到了正确的位置。从而在递归排序时每次划分都满足上述性质，合并后可以完成整个数组的排序。 使用快速排序算法针对数组进行升序排序，以最右边元素作为主元，请写出针对（600, 500, 400, 300, 200, 100）这个输入实例进行排序的执行过程。 简述分治法与平衡的关系，并阐述你对快速排序算法中平衡的理解。 分治与平衡密不可分，具有平衡结构的分治效率往往更高。 × × √ √ × × √ × √ √ × × × × √ ×（不含等号） 不可以（包含关系） 有些问题无法在多项式时间求精确解，但可以求近似解 check-4 check-5 check-6 模拟算法： 模拟就是用计算机来模拟题目中要求的操作。模拟题目通常具有码量大、操作多、思路繁复的特点。由于它码量大，经常会出现难以查错的情况，如果在考试中写错是相当浪费时间的。 近似算法： 在计算复杂性理论中的某些假设下，比如最著名的 假设下，对于一些可已被证明为NP完全的优化问题，无法在多项式时间内精确求到最优解，然而在现实或理论研究中，这类问题都有广泛的应用，在精确解无法得到的情况下，转而依靠高效的近似算法求可以接受的近似解。近似算法的研究也是当今计算机科学研究的一个主要方向。（有近似精度的要求） 随机算法： 在算法的过程中引入随机数，使得算法在执行的过程中随机选择下一个计算步骤。它最后可能导致结果也是不确定的。一个结果不确定的概率算法叫做 Monte Carlo 算法，而总是得到准确解的概率算法叫做 Sherwood 算法（一个例子是引进随机因子的快速排序算法）。 Master theorem Amortized analysis 聚合分析（Aggregate Analysis）通过计算一系列操作的总成本，并将其平均到每次操作上，从而得出每次操作的均摊时间复杂度。 以动态数组为例，首先，可以得到插入操作的两个关键成本： 如果数组未满，插入操作的成本为 。 如果数组已满，则插入操作需要扩容，扩容后复制元素的成本为 ，其中 为当前数组的大小。 所以，为了计算 n 次插入操作的总成本，可以将其分开为两部分计算： 插入操作的成本：每次插入新元素的直接成本是常数时间 ，对于 次操作，总成本是 。 数组扩容的成本：每次扩容涉及到复制原数组元素到新数组。这些操作发生在数组大小为 的时刻，其中 是小于等于 的最大幂。扩容操作的成本分别是 ，总和为 ，这是一个等比数列的和，其结果为 。 因此，该数组总的插入成本为 ，均摊到每次操作的成本为 。即使在最坏情况下，平均每次插入操作的成本依然是常数时间。 NP problems P类问题： 在确定型图灵机上能使用多项式时间的算法得到该问题的 解（显然能在多项式时间内对问题进行验证） NP类问题： 在非确定型图灵机上能使用多项式时间进行 验证 的问题（在现有算法下不一定能以多项式时间解决） - P类问题是NP类问题的子集 - 例子：旅行商问题TSP 即有一个推销员，要到n个城市推销商品，他要找出一个包含所有n个城市的环路，这个环路路径小于a 问题约化： 可以用问题B的算法来解决A ，我们就说问题A可以约化成问题B。（二元一次方程的解法可以用于解一元一次方程）约化具有传递性。存在最大的问题。 NPC类问题： 存在这样一个NP问题，所有的NP问题都可以约化（多项式规约）成它。换句话说，只要解决了这个问题，那么所有的NP问题都解决了。其定义要满足2个条件： 首先，它得是一个NP问题；然后，所有的NP问题都可以约化到它。 NP-Hard问题： NP-Hard问题是这样一种问题，它满足NPC问题定义的第二条但不一定要满足第一条（就是说，NP-Hard问题要比 NPC问题的范围广，NP-Hard问题没有限定属于NP），即所有的NP问题都能约化到它，但是它不一定是一个NP问题。 np Divide and Conque Prob-1 设X[0:n-1]和Y[0:n-1]为两个数组，每个数组中含有n个已排好序的数。试设计一个O(logn)时间的分治算法，找出X和Y的2n个数的中位数，并证明算法的时间复杂性为O(logn) 问题同上，只是X[0:m-1]和Y[0:n-1]，两有序数组长度不同，试设计一个O(log(m+n))时间的分治算法 两个数组长度相等时： 算法：考虑 数组中间位置的值 ，若 ，则说明 位于整个 个值的后半部，因此中位数在 和 之中；若 ，则说明 位于所有 个数的前半部，因此中位数在 和 之中；否则 就是中位数。若上一步没有找到中位数，则从 数组开始查找中间值，将中间值与 数组对应位置的值比较。递归上述步骤直到找到中位数。 复杂度：最好情况是一次寻找就能找到中位数 ，最坏情况是每次只能去掉 的规模的数据，复杂度公式为 ，根据 Master Theorem， 两个数组长度不等时： Prob-2 123456789101112131415161718192021222324252627282930def merge(self, left, right): # 记录目前左右建筑物的高度 lheight = rheight = 0 # 位置 l = r = 0 # 输出结果 res = [] while l &lt; len(left) and r &lt; len(right): if left[l][0] &lt; right[r][0]: # current point cp = [left[l][0], max(left[l][1], rheight)] lheight = left[l][1] l += 1 elif left[l][0] &gt; right[r][0]: cp = [right[r][0], max(right[r][1], lheight)] rheight = right[r][1] r += 1 # 相等情况 else: cp = [left[l][0], max(left[l][1], right[r][1])] lheight = left[l][1] rheight = right[r][1] l += 1 r += 1 # 和前面高度比较，不一样才加入 if len(res) == 0 or res[-1][1] != cp[1]: res.append(cp) # 剩余部分添加进去 res.extend(left[l:] or right[r:]) return res Prob-3 最大子数组问题。一个包含n个整数（有正有负）的数组A，设计一O(nlogn)算法找出和最大的非空连续子数组。对于此问题你还能设计出O(n)的算法吗？ - 例如：[0, -2, 3, 5, -1, 2]应返回9，[-9, -2, -3, -5, -3]应返回-2 见DP Prob-4 循环移位问题。给定一个数组，数组中元素按从小到大排好序，现将数组中元素循环右移若干位，请设计一算法，计算出循环右移了多少位。 分治，合并时检查是否存在&gt;的数，若存在直接返回，位数即栈中的偏移量加上合并发现&gt;的位置 Prob-5 两元素和为 X。给定一个由 n 个实数构成的集合 S 和另一个实数x，判断 S 中是否有两个元素的和为 x。试设计一个分治算法求解上述问题，并分析算法的时间复杂度。 遍历加二分查找：先排序，复杂度 遍历每一个实数复杂度为 ，在剩余未遍历的集合中查找 ，复杂度 。总复杂度 。 Prob-6 有一实数序列𝑎_1,𝑎_2,…,𝑎_𝑁，若𝑖&lt;𝑗 且 𝑎_𝑖&gt;𝑎_𝑗，则(𝑎_𝑖,𝑎_𝑗)构成了一个逆序对，请使用分治方法求整个序列中逆序对个数，并分析算法的时间复杂性。 - 例如：序列(4,3,2)逆序对有(4,3)，(4,2)，(3,2)共3个 归并排序统计逆序对。在合并时注意：若左半部分大于右半某个值，则左半部分后续的值都大于它。 merge Prob-7 给定 个区间，求它们的最大交区间。 Dynamic Programming 可以在多项式时间内解决问题 Prob-1 给出N个1-9的数字 (v1,v2,…,vN)，不改变它们的相对位置，在中间加入K个乘号和N-K-1个加号，（括号随便加）使最终结果尽量大。因为乘号和加号一共就是N-1个了，所以恰好每两个相邻数字之间都有一个符号。说明其具有优化子结构性质及子问题重叠性质。 - 例如： N=5, K=2，5个数字分别为1、2、3、4、5，可以加成： - 1 * 2 * (3 + 4 + 5) = 24 - 1 * (2 + 3) * (4 + 5) = 45 - (1 * 2 + 3) * (4 + 5) = 45 优化子结构性质 优化子结构性质意味着问题的最优解包含其子问题的最优解。对于这个问题，我们可以将整个序列分为两部分，分别求解这两部分的最大值，然后将这两部分的最大值通过加号或乘号连接起来，以获得整个序列的最大值。 子问题重叠性质 子问题重叠性质意味着在递归求解过程中，相同的子问题会被多次计算。在这个问题中，当我们计算不同分割点的最大值时，很多子序列会被重复计算，因此我们可以将这些子序列的结果存储起来，避免重复计算。 动态规划解决方案 我们可以定义一个二维数组 dp[i][j] 表示从第 i 个数字到第 j 个数字之间插入乘号和加号能得到的最大值。我们的目标是计算 dp[0][N-1]。 状态转移方程 对于任意的 i 和 j（i &lt; j），我们有两种选择： 在 i 和 j 之间插入一个加号，那么问题就变成了求解 dp[i][k] 和 dp[k+1][j] 的最大值，其中 k 是 i 和 j 之间的某个分割点，然后我们将这两个结果相加：dp[i][j] = max(dp[i][j], dp[i][k] + dp[k+1][j])。 在 i 和 j 之间插入一个乘号，那么问题就变成了求解 dp[i][k] 和 dp[k+1][j] 的乘积，其中 k 是 i 和 j 之间的某个分割点：dp[i][j] = max(dp[i][j], dp[i][k] * dp[k+1][j])。 我们需要枚举所有可能的 k 来更新 dp[i][j]。 初始化 dp[i][i] = v[i]，因为只有一个数字，不需要任何操作。 计算顺序 我们按照 j - i 的长度从小到大计算 dp[i][j]，这样可以确保在计算 dp[i][j] 时，其子问题 dp[i][k] 和 dp[k+1][j] 已经被计算过。 返回值 最终，dp[0][N-1] 将给出整个序列的最大值。 Prob-2 给定一长度为N的整数序列(a1,a2,…,aN) ，将其划分成多个子序列（此问题中子序列是连续的一段整数），满足每个子序列中整数的和不大于一个数 B，设计一种划分方法，最小化所有子序列中最大值的和。说明其具有优化子结构及子问题重叠性质 例如： 序列长度为8的整数序列(2,2,2,8,1,8,2,1)，B=17，可将其划分成三个子序列(2,2,2)，(8,1,8)以及(2,1)，则可满足每个子序列中整数和不大于17，所有子序列中最大值的和12为最终结果。 考虑子问题： ， 显然当存在 有更小值时，将该值替换问题的最优解，可以得到更优解，因此原问题最优解不成立而矛盾，所以该问题具有最优子结构。而在计算 时由于要遍历 而产生重复值计算的值，因此具有子问题重叠性质。 Prob-3 对一棵树进行着色，每个结点可着黑色或白色，相邻结点不能着相同黑色，但可着相同白色。令树的根为r，请设计一种算法对树中尽量多的节点着黑色。 只有两种情况：（1）根节点为黑色 0，子节点只能为白色 1，（2）根节点为白色，子节点可黑可白， 维护DP表。递推公式是 当节点是叶子节点时， Prob-4 在自然语言处理中一个重要的问题是分词，例如句子“他说的确实在理”中“的确”“确实”“实在”“在理”都是常见的词汇，但是计算机必须为给定的句子准确判断出正确分词方法。一个简化的分词问题如下：给定一个长字符串y=y1y2…yn，分词是把y切分成若干连续部分，每部分都单独成为词汇。我们用函数quality(x)判断切分后的某词汇x=x1x2…xk的质量，函数值越高表示该词汇的正确性越高。分词的好坏用所有词汇的质量的和来表示。例如对句子“确实在理”分词，quality(确实) + quality(在理) &gt; quality(确)+quality(实在)+quality(理)。请设计一个动态规划算法对字符串y分词，要求最大化所有词汇的质量和。（假定你可以调用quality(x)函数在一步内得到任何长度的词汇的质量） 递推公式： Prob-5 给定 𝑛 个活动，活动𝑎_𝑖表示为一个三元组(𝑠_𝑖,𝑓_𝑖,𝑣_𝑖)，其中𝑠_𝑖表示活动开始时间，𝑓_𝑖表示活动的结束时间，𝑣_𝑖表示活动的权重。带权活动选择问题是选择一些活动，使得任意被选择的两个活动𝑎_𝑖和𝑎_𝑗执行时间互不相交，即区间[𝑠_𝑖,𝑓_𝑖]与[𝑠_𝑗,𝑓_𝑗]互不重叠，并且被选择的活动的权重和最大。请设计一种方法求解带权活动选择问题。 先将活动按照结束时间排序，定义最早结束的活动到第 个结束的活动的最优排列方案是 ，结束时间早于 的活动集合的最优排列方案为 ，则 满足最优子结构： 重叠子问题集合： 空间复杂度 ，时间复杂度 Prob-6 受限最短路径长度问题：给定一无向图G=(V, E, A, B)，A(e)表示边e的长度，B(v)表示顶点v的花费，计算小明从顶点s到顶点d的最短路径长度，满足以下限制，初始时小明随身携带M元钱，每经过一个顶点v，须交B(v)的过路费，若身上有大于B(v)的钱则可以通过，否则不可以通过。求顶点s到顶点d的最短路径 递推公式： 然后BFS从开始节点遍历子节点，注意记忆化存储 Prob-7 给定𝑛个物品，每个物品有大小𝑠_𝑖，价值𝑣_𝑖。背包容量为𝐶。要求找到一组物品，这些物品整包完全占满背包容量𝐶，且总体价值最大。请写出动态规划迭代公式。 设 表示装入前 个物品时，容量为 的背包的最大总价值。 递推公式： 或者 每个物品只能放一次，因此是0-1背包，注意内层循环需要反向，最后需要检查背包是否占满（==） Prob-8 最大子数组问题：一个包含n个整数（有正有负）的数组A，设计一O(nlogn)算法找出和最大的非空连续子数组。（例如：[0, -2, 3, 5, -1, 2]应返回9，[-9, -2, -3, -5, -3]应返回-2。） DP解法： 递推公式：，找到DP数组，再找DP数组的最大值 DC解法：合并跨越中线的数组时遍历即可，注意左边是 右边是 。 Prob-9 最长非降子序列：一个序列有N个数：A[1],A[2],…,A[N]，求出最长非降子序列的长度。 对于长度为 的子序列的最后一个元素的最小值 ，如果新来的元素大于它，就插入到 数组末尾（说明长度为 时，子序列最后一个元素最小是这个新来的元素）；如果小于它，则用二分查找，找第一个大于新元素的，用新元素 替换 它 （假设找到的位置是 即在不降子序列长度为 时，末尾的元素还能更小）。 对于最长 上升 子序列问题，类似地，可以令 d_i 表示所有长度为 i 的最长上升子序列的末尾元素的最小值。 需要注意的是，若 ，由于最长上升子序列中相邻元素不能相等，需要在 d 序列中找到 第一个 不小于 的元素，用 替换之。 Greedy Prob-1 给定n个物品，物品价值分别为P1，P2，…，Pn，物品重量分别W1，W2, …, Wn，背包容量为M。每种物品可部分装入到背包中。输出X1，X2，…，Xn，0&lt;=Xi&lt;=1, 使得 最大，且 。试设计一个算法求解该问题，分析算法的正确性。 Prob-2 海面上有一些船需要与陆地进行通信，需要在海岸线上布置一些基站。现将问题抽象为，在x轴上方，给出N条船的坐标𝑝_1,𝑝_2,…,𝑝_𝑁，𝑝_𝑖=(𝑥_𝑖,𝑦_𝑖)，𝑥_𝑖≥0, 𝑦_𝑖≤d,1≤𝑖≤𝑁，在x轴上安放的基站可以覆盖半径为d的区域内的所有点，问在x轴上至少要安放几个点才可以将x轴上方的点都覆盖起来。试设计一个算法求解该问题，并分析算法的正确性。 Prob-3 某公司有个工厂和仓库。由于原材料等价格波动，工厂每个月的生产成本也会波动，令第𝑖个月产品的单位生产成本为𝑐_𝑖（该月生产一个产品的成本为𝑐_𝑖）。仓库储存产品的也有成本，假设每个月产品的单位储存成本为固定值1（存储一个产品一个月的成本为1）。令第𝑖个月需要供应给客户的产品数量为𝑦_𝑖，仓库里的和生产的产品均可供应给客户。假设仓库的容量无限大，供应给客户剩余的产品可储存在仓库中。若已知𝑛个月中各月的单位生产成本𝑐_𝑖、以及产品供应量𝑦_𝑖，设计一算法决策每个月的产品生产数量𝑥_𝑖，使得𝑛个月的总成本最低。例如：𝑛=3，𝑐_𝑖:2,5,3，𝑦_𝑖:2,4,5，则𝑥_𝑖:6,0,5，即第1个月生产6个供应2个（代价2×2=4），储存4个供应给第2个月（代价(2+1)×4=12），第3个月生产5个供应5个（代价3×5=15），使总成本4+12+15=31最小 Prob-4 给定直线上 2n个点的序列P[1,2,… ,2n]，每个点 P[i]要么是白点要么是黑点，其中共有n个白点和 n个黑点，相邻两个点之间距离均为1，请设计一个算法将每个白点与一黑点相连，使得连线的总长度最小。例如，图中有4个白点和4个黑点，以图中方式相连，连线总长度为1+1+1+5=8。 Prob-5 有n个作业需要在一台机器上执行，一个时刻机器上只能执行一个作业，每个作业可在单位时间内完成，作业i有截止时间di，当作业i在截止时间被执行完，则可获得pi的收益，请设计算法获得最大收益，并分析算法的正确性 Prob-6 假设有数目不限的面值为25美分，10美分，5美分，1美分的硬币，请使用最少个数的硬币凑出3.33美元。 Searching Prob-1 请从给定的点数网格𝒂[𝟏,𝟐,…,𝟕][𝟏,𝟐,…,𝟖]使用搜索算法求出对应 的骨牌号图𝒃[𝟏,𝟐,…,𝟕][𝟏,𝟐,…,𝟖]，有可能的话，给出相关剪枝策略。 Network flow/Bipartition matching Ford-Fulkerson alg 适用于稠密图，可以用DFS找增广路，时间复杂度为 ， 是最大流量： 搜索出一条增广路； 在这条路径中所有的边容量减去这条增广路的流量，并建立流量为增广路增加流量相反数的反向边； 返回操作一，如果没有增广路则得到答案 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * 查找增广路 * @param c 当前节点 * @param t 汇点 * @param f 当前路径中的容量最小值 * @return */int dfs(int c, int t, int f) { // 如果当前节点是汇点 t，直接返回容量最小值，即增广路增加的流量 if (c == t) { return f; } // 记忆化搜索染色 used[c] = true; // 遍历 c 节点下一个节点 for (int i = 0; i &lt; G[c].size(); ++ i) { Edge &amp;e = G[c][i]; // 如果这个节点未被访问到，并且其当前容量大于 0 if (!used[e.to] &amp;&amp; e.cap &gt; 0) { // 访问到最深层节点 int d = dfs(e.to, t, min(f, e.cap)); if (d &gt; 0) { // 当前边容器减少 e.cap -= d; // 反向边容量增加 G[e.to][e.rev].cap += d; return d; } } } return 0;}int max_flow(int s, int t) { int flow = 0; int cnt = 0; for (;;) { memset(used, 0, sizeof(used)); int f = dfs(s, t, INF); cnt += 1; if (f == 0) { cout &lt;&lt; cnt &lt;&lt; endl; return flow; } flow += f; }} Edmond-Karp alg 适用于稀疏图，用BFS找增广路，算法复杂度为 ： 使用 BFS 找到一条增广路（对应下面的步骤 1）； 计算这条路的最小容量边，为汇点加流量，并建立反向边，其容量为增加的流量（对应下面的步骤 2）； 重复第一步，如果不能找到一条增广路则得到最大流； 但是在实现上，由于我们采用了 BFS 方法，则无法对这条增广路进行回溯处理。所以在代码实现的时候，我们需要通过一个数组或者一个 Map 来记录下对应点在增广路上的入度边 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 用来记录当前路径上的最小容量，用于加流量操作int a[MAX_V]; // 记录下标点的边编号，pair 对应 G[x1][x2]，x1 是描述哪个入度点，x2 是描述 x1 点的第 x2 条边unordered_map&lt;int, pair&lt;int, int&gt;&gt; pre; void bfs(int s, int t) { // a 初始化成 0，也可以判断是否已经被染色，从而剪枝情况 memset(a, 0, sizeof(a)); // 使用队列，保存处理节点 queue&lt;int&gt; que; que.push(s); // 每个节点所流过的流量设置为 INF 无穷大 // 这样可以起到求最小的作用 a[s] = INF; while (!que.empty()) { int x = que.front(); que.pop(); // 遍历当前节点的所有边 for (int i = 0; i &lt; G[x].size(); ++ i) { Edge&amp; e = G[x][i]; // 如果相连的点没有访问，并且这条边的容量大于 0 if (!a[e.to] &amp;&amp; e.cap &gt; 0) { // 记录下一个点的入度边 pre[e.to] = make_pair(x, i); // 计算当前路径的最小容量 a[e.to] = min(a[x], e.cap); que.push(e.to); } } if (a[t]) break; }}int max_flow(int s, int t) { // 最大流结果 int ret = 0; while (1) { // 从 S -&gt; T 使用 bfs 查询一条增广路 bfs(s, t); // 如果发现容量最小是 0 ，说明查不到了 if (a[t] == 0) break; int u = t; while (u != s) { // 使用 pre 来获取当前增广路中汇点 T 的入度边下标信息 int p = pre[u].first, edge_index = pre[u].second; // 获取正向边和反向边 Edge&amp; forward_edge = G[p][edge_index]; Edge&amp; reverse_edge = G[forward_edge.to][forward_edge.rev]; // 更新流量 forward_edge.cap -= a[t]; reverse_edge.cap += a[t]; // 逆增广路方向移动游标继续更新 u = reverse_edge.to; } ret += a[t]; } return ret;} Prob-1 Graph Topological sorting Activity on Vertex Network (AOV) 顶点表示活动。当活动的所有前驱节点都完成，该节点才能执行。 拓扑序列 构造如下 从图中选择入度为0的点 输出该顶点，然后从图中删除此顶点的所有出边 重复上述步骤直到所有顶点都输出，拓扑排序完成；否则图中不存在入度为0的点，是有环图，死锁。 Activity on Edge Network (AOE) 边表示活动/权值/时间，顶点表示事件。 事件（顶点）的最早发生时间 即需要所有前驱节点完成，是源点到该节点的最大路径 事件（顶点）的最迟发生时间 事件的所有后继活动的最迟开始时间的最小值 活动（边）的最早发生时间 活动（边）的最迟发生时间 后继事件的最迟发生时间 - 该事件的持续时间（权值） 关键路径：AOE 网中从源点到汇点的最长路径的长度。 关键活动：即关键路径上的活动，它的最早开始时间和最迟开始时间相等。 递推求最早和最迟发生时间： 按拓扑顺序求，最早发生时间从前往后递推，最迟发生时间从后往前递推 Kaha算法 初始状态下，集合 S 装着所有入度为 0 的点，L 是一个空列表。 每次从 S 中取出一个点 u（可以随便取）放入 L, 然后将 u 的所有边 删除。对于边 ，若将该边删除后点 v 的入度变为 0，则将 v 放入 S 中。不断重复以上过程，直到集合 S 为空。检查图中是否存在任何边，如果有，那么这个图一定有环路，否则返回 L，L 中顶点的顺序就是构造拓扑序列的结果。 时间复杂度： Minimum spanning tree (MST) （m是边数，n是点数） Kruskal：维护一个森林，查询两个结点是否在同一棵树中，连接两棵树。时间复杂度 kruskal Prim：每次要选择距离最小的一个结点，以及用新的边更新其他结点的距离。时间复杂度 （暴力查找）， （二叉堆） prim Minimum path 性质 对于边权为正的图，任意两个结点之间的最短路，不会经过重复的结点。 对于边权为正的图，任意两个结点之间的最短路，不会经过重复的边。 对于边权为正的图，任意两个结点之间的最短路，任意一条的结点数不会超过 n，边数不会超过 n-1。 Floyd算法： 可以求任意两个结点之间的最短路。适用于任何图，不管有向无向，边权正负，但是最短路必须存在。（不能有个负环） 设 f[k,x,y] 表示只允许经过节点 1 到 k 的 x 到 y 的最短路。则最终要求的是 f[n,x,y]。f[0,x,y]=0 or inf f[k,x,y] = min(f[k-1,x,y], f[k-1,x,k] + f[k-1,k,y])：分成是否经过点 k 来考虑 第一维对于结果无影响 时间复杂度 ，空间复杂度 1234for (k = 1; k &lt;= n; k++) for (x = 1; x &lt;= n; x++) for (y = 1; y &lt;= n; y++) f[x][y] = min(f[x][y], f[x][k] + f[k][y]); Bellman-Ford算法：可以处理负权图，可以判断最短路是否存在。复杂度 [bellman-ford]bellman-ford.png) 需要注意的是，以 S 点为源点跑 Bellman–Ford 算法时，如果没有给出存在负环的结果，只能说明从 S 点出发不能抵达一个负环，而不能说明图上不存在负环。 因此如果需要判断整个图上是否存在负环，最严谨的做法是建立一个超级源点，向图上每个节点连一条权值为 0 的边，然后以超级源点为起点执行 Bellman–Ford 算法。 Dijkstra算法：求解非负权图单源最短路。 将结点分成两个集合：已确定最短路长度的点集（记为 S 集合）的和未确定最短路长度的点集（记为 T 集合）。一开始所有的点都属于 T 集合。初始化 dis(s)=0，其他点的 dis 均为 。然后重复这些操作： 从 T 集合中，选取一个最短路长度最小的结点，移到 S 集合中。 对那些刚刚被加入 S 集合的结点的所有出边执行松弛操作。 直到 T 集合为空，算法结束。 时间复杂度：稀疏图 二叉堆实现更优；稠密图 暴搜优于二叉堆。 123456789101112131415161718192021struct edge { int v, w;};vector&lt;edge&gt; e[MAXN];int dis[MAXN], vis[MAXN];void dijkstra(int n, int s) { memset(dis, 0x3f, (n + 1) * sizeof(int)); dis[s] = 0; for (int i = 1; i &lt;= n; i++) { int u = 0, mind = 0x3f3f3f3f; for (int j = 1; j &lt;= n; j++) if (!vis[j] &amp;&amp; dis[j] &lt; mind) u = j, mind = dis[j]; vis[u] = true; for (auto ed : e[u]) { int v = ed.v, w = ed.w; if (dis[v] &gt; dis[u] + w) dis[v] = dis[u] + w; } }} Bipartite graph 性质 如果两个集合中的点分别染成黑色和白色，可以发现二分图中的每一条边都一定是连接一个黑色点和一个白色点。 二分图不存在长度为奇数的环（可以用DFS/BFS遍历图，不存在奇数环的连通图则是二部图） 最大匹配 给定二分图，要求选定一些边，使得边之间没有公共顶点，且边的数量最大。 交替路/增广路：从未匹配的点出发，依次交替经过非匹配边、匹配边，到达另一个非匹配点。然后对增广路上所有的非匹配和匹配取反，得到新的匹配图。（DFS，） 转为最大流：源点连接左边集合，汇点连右集合，容量都为1 ----","categories":[{"name":"Lecture","slug":"Lecture","permalink":"https://aimecreative.github.io/categories/Lecture/"}],"tags":[{"name":"Lecture","slug":"Lecture","permalink":"https://aimecreative.github.io/tags/Lecture/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://aimecreative.github.io/tags/Algorithm/"}],"author":"Mecreative"},{"title":"AICompiler | AI Framework","slug":"Compiler/ai_frame","date":"2024-09-02T02:57:08.000Z","updated":"2024-09-02T11:30:55.035Z","comments":true,"path":"2024/09/02/Compiler/ai_frame/","link":"","permalink":"https://aimecreative.github.io/2024/09/02/Compiler/ai_frame/","excerpt":"","text":"AI 框架 前端：面向用户的编程语言和接口。C++，Python，Lua等 统一表示：神经网络的中间表示、计算图 优化层： 自动微分 计算图优化 运行时：内存管理、计算图调度和执行 底层库： 内核代码的优化和编译 多硬件支持","categories":[{"name":"Compiler","slug":"Compiler","permalink":"https://aimecreative.github.io/categories/Compiler/"}],"tags":[{"name":"Compiler","slug":"Compiler","permalink":"https://aimecreative.github.io/tags/Compiler/"}],"author":"Andrew-Rey"},{"title":"AICompiler | Auto Differentiation","slug":"Compiler/auto_grad","date":"2024-09-01T02:53:32.000Z","updated":"2024-09-01T07:51:49.708Z","comments":true,"path":"2024/09/01/Compiler/auto_grad/","link":"","permalink":"https://aimecreative.github.io/2024/09/01/Compiler/auto_grad/","excerpt":"","text":"微分 综述：机器学习中的自动微分 张量自动微分 关于自动微分的编程系统 符号微分：直接使用链式法则对表达式求导。优点是数值精确，缺点为表达式膨胀 数值微分：使用有限差分求解： 数值微分会由于计算机中不同浮点数精度（FP16, FP32, FP64）而造成截断误差（truncation error）和舍入误差（round-off error） 自动微分： 所有数值运算都由有限的基本运算构成 基本运算的导数表达式已知 通过链式法则链接将数值计算各部分组合为整体 自动微分中的 表达式追踪（Evaluation trace） 能追踪数值计算过程中的中间变量。以下述函数为例 表示为计算图为 computation_graph 前向微分 (forward mode) 从输入出发，逐步使用链式法则进行求导。 forward 通过一个输入可以求解所有的输出微分，不适用与深度学习多输入少输出的情况 后向微分 (backward mode) 对于每一个结点，求解最终输出对该结点的导数。注意每一次求导需要用到后继相邻结点的导数。 reverse 可以一次性求解所有输入的微分，但需要大量存储空间存储中间结果的微分 Jacobian 矩阵 前向和后向微分都可以使用 Jacobian 矩阵来表示：对于函数 称为原函数的 Jacobian 矩阵。其中每行为后向微分 (Adjoint) 的求解结果，每列为前向微分 (Tangent) 的求解结果。 输出数量小于输入时，常用后向微分方法；反之前向。 自动求导实现方法 LIB：基本表达式，封装基本的表达式及其微分表达式作为库函数，运行时记录基本表达式和相应的组合方式，使用链式法则对基本表达式的微分结果进行组合 OO：操作符重载，利用语言多态，使用操作符重载基本表达式运算符，其余类似LIB，代表库为PyTorch AST：源码转换，语言预处理、编译器或解释器的扩展，对程序表达进行分析得到基本表达式的组合方式，代表库为MindSpore","categories":[{"name":"Compiler","slug":"Compiler","permalink":"https://aimecreative.github.io/categories/Compiler/"}],"tags":[{"name":"Compiler","slug":"Compiler","permalink":"https://aimecreative.github.io/tags/Compiler/"}],"author":"Andrew-Rey"},{"title":"致谢 | 我经历过的微风、夕阳和大海","slug":"You/acknowledgement","date":"2024-05-30T11:50:33.000Z","updated":"2024-06-20T18:14:09.621Z","comments":true,"path":"2024/05/30/You/acknowledgement/","link":"","permalink":"https://aimecreative.github.io/2024/05/30/You/acknowledgement/","excerpt":"","text":"2020级SEU本科毕业致谢篇。谨以此文，献给自己。 很多年过去，我还是喜欢那一座城市 在内心毫无波澜甚至有些许失望的情况下接受了高考的第三志愿的第二专业：SEU吴健雄学院。填写志愿时并无对任何专业有过多的了解，凭本事选择了大类培养的吴健雄学院。同样，对于未来的大学生活我也没有任何期待和了解。我从午休的床上起来，正值盛夏，在湖中心。民宿的周围有野花盛开，清澈湛蓝的水波轻拍岸沿。阳光也是懒散的。 几周前我结束了长沙之行。长沙城是一座内陆城市，通过湘江与长江相接，在高中时便经常背诵“君住长江头，妾住长江尾。夜夜思君不见君，共饮长江水”，如今站在橘子洲头，确实能感受到四面奔腾的湘江水带来的无尽的哀愁。去逛了博物馆和美术馆，我还是觉得美术馆更适合我，不同的心情可以有不同的思考，没有答案的作品可以让我进行二次创作，我是自由而孤独的。美术馆的灯光打在红色的墙上，我拍下了黑色的影子。但随后我删了，因为我以为以后还能再见。茶颜悦色在这座城市开得到处都是，第一次被称呼“许小主”的心情还带着些许奇妙，喝了很多饮品，我还是喜欢幽兰拿铁，以至于上大学后在网易云中去搜幽兰拿铁，竟然还能搜到相关的歌曲。不过后来，南京也有了茶颜，但我已经没有了那种好奇和兴奋的感觉。当然，在彻底爱上这座城市前，也无法忘记那一天的炎热，与当时自己乘公交前往一个偏远的地方时的稚嫩与愚笨。博物馆和艺术馆的照片仍然存在手机相册里，茶颜悦色和文和友消失在夜晚的草地。热闹的街景，谁与谁又并肩牵手穿梭于夏天的夜晚与记忆的牢笼。 牢笼仍在，我却很久没有回到长沙城。 长沙 但是接到志愿通知的时候我却无法开心。我会回想起高中夏日粉色和紫色的夕阳与金色的树黑色的鸟，一声吱呀，定格了无法回去的夏天。18年的分号停留在了通知书寄到手边的那一刻。 随之而来的是死寂般的沉默与一个月的暑假和三次前往同一片湖度假。 就是我现在看着的这片湖。我试着在深夜拍摄星轨，我喜欢积雨云中倾泻的阳光。 一个月后的南京，至今竟然已经蜗居四年之久。 星期三，南京大雨 带着大一新生独有的清澈心灵与眼眸，与父母在东南大学北门告别后，我踏入陌生的校园。只不过再次转身时，已经看不见他们了。 仍然无法确定中学是否结束，18岁是否已过。在整理床铺的时候突然会想起在家旁边的公园里，LED灯中有人弹着吉他，我竟然有点想哭。我才明确地知道，这里是南京，是名为东南大学的校园，是桃园宿舍。在公众号中出现过的健雄书院如今就在我眼前。即使很累，我还是带着我的通知书去了各种东大的打卡地。——图书馆和大草坪，教学楼，九龙湖。坐在图书馆的台阶上，无人分享和倾诉。有时有人从图书馆出来，有时有人前往图书馆。我感到劳累和疲惫，在陌生的阳光下。 于是车载着我没头没脑地开始往前开，蛮横地冲撞时间，我却把记忆忘在了起点，忘在了长沙南站的分别。都说被记住的才是生活，在下车时，我开始寻找自己的生活。但我撞不开时间。我回不去起点。 要我说，大一堆砌的是中值定理与各种积分。在周围人的口算题中绞尽脑汁，在月光下一个人回寝。我很早就知道了6号和8号教学楼三四楼不锁门不关灯，于是我经常在里面宅一天。我不爱回寝室，即使在偌大的阶梯教室里望着窗外，如笼中之鸟。开始熬夜，开始享受这种没有人打扰的思考与独处，开始沉默，开始戴着耳机听歌。最常见的景色就是夕阳照到交通楼，呈现金黄，或是天空出现粉橙的脸颊，以及写满的草稿纸，和没有结果的题目。我明白我陷入了一个神秘而可怕的深渊之中，但我不能自救。我强迫自己去散步，不允许沉沦。 当时最开心的还有梅园操场上一排金色的银杏，一周后被砍了。剩下灰色的枯叶和惨白的天空。以及即将到来的冬天。 银杏 我开始记录一些自己的想法。在一些人看来，写日记就是在欺骗自己。但是我喜欢那一本小王子封面的日记本，小王子、B612和一朵玫瑰以及44次日落。有一个人对我说： 压力都是自己给的，不用和别人比，朝着自己的方向走吧。时间管理是你最擅长的。大学每个人的放心都不一样，没有必要因为别人怎么样自己就怎么样。放弃一些让自己不舒服的社交圈，实在憋得喘不过气就不用我告诉你怎么做吧。 每个人都在黑暗中探索，你前面有很多人，没有办法，你只能跟着拿蜡烛的他们。 但是你有没有想过，你可以自己拿着蜡烛去自己想去的地方。 我不管那个人懂不懂我的劳累，我的压力和孤独。但那个人听完了。 我没有办法去草地上听乐队演奏，即使夕阳很美。悔恨吗，或许吧，一把跨越七年的刀刃插进了不成熟的内心。我没有想过我可以自己拿着蜡烛去自己想去的地方。或许想过。或许一定想过。如今却站在了四年的末尾，回望四年或七年的所有是非，有时候能看见一点光亮。 一个人和一群人 我总觉得应该去找寻或者思考一些什么，历史也好，哲学也罢。只有思考时，我才能感受到自己存在于世界，双脚真正站在陆地上，双眼看着飞鸟。一昧的自责和反思只会加剧自卑感和乏力感，但当你开始把视线投向更远的远方时，才能感受到当下的存在和历史的实体。当时自己的内心一定是烦躁不安的，我希望去改变一些什么，任何方向都行。因为最差的结果是维持现状。让我印象颇为深刻的是疫情延迟放假，我在第一次在图书馆度过夏天。喜欢那里傍晚的鸟鸣，有薄荷的味道和天空的清澈。更重要的是让我莫名其妙增加了在计算机上的兴趣和自信。这种自以为是让我选择了AI和CS作为专业。 后来我一个人去了北京。竟然真的能在早晨的四合院中看见逗鸟的大爷。一个人爬完长城南北两段后无法忘记在烽火台上清爽的风。租一辆车骑完整条长安街。喝一碗豆汁儿。在蓝调的白塔公园感受寒冷和肃杀。南锣鼓巷不长但终于看见一点烟火气。 雨，雾 乌鸦的叫声 马路上飞驰的车辆 护城河，一浪接一浪 秋也是凉的 世事一场大梦 红色的宫墙又在上漆 凹凸不平的石砖有点硌脚 五千年一晃而过 游客还在拍照 已经忘了很多事情。也忘了暑假结束后是为何来到书院并加入其中成为志愿者的。“海看多了想见人，人见多了想看海”。于是命运的齿轮开始转动。 我很少对已经发生的事情做一些相反的设想，因为这些设想的目的往往是加强对当下的信念感，但我还是无法想象如果我当初没有来到书院，我现在在哪，后续的两年又会如何度过，或者会更优秀，或者会更差劲。唯一清楚的是，人是群居动物，至少我无法长时间忍受失去交流的生活。很多事情很多矛盾很多情绪，见一面就好了。除非累了。一个人的价值是通过“被需要”体现的，这种“被需要”在我第一次来书院时展现得淋漓尽致，因为我否定自己太多了。虽然还是不怎么回寝室，但终于不是一个人在教室最后一排看着夕阳发呆了。对于所谓大学的记忆也开始变得有迹可循。书院、校队和活动室都成为了我的大学。 现在的故事 后来的我","categories":[{"name":"You","slug":"You","permalink":"https://aimecreative.github.io/categories/You/"}],"tags":[{"name":"You","slug":"You","permalink":"https://aimecreative.github.io/tags/You/"}],"author":"Andrew-Rey"},{"title":"HPC | Modern Architecture-Cache","slug":"HPC/modern_arch","date":"2024-04-23T13:01:02.000Z","updated":"2024-07-28T09:46:29.398Z","comments":true,"path":"2024/04/23/HPC/modern_arch/","link":"","permalink":"https://aimecreative.github.io/2024/04/23/HPC/modern_arch/","excerpt":"Parallel Programming: Concepts and Pracitce - Chapter 3","text":"Parallel Programming: Concepts and Pracitce - Chapter 3 von Neumann bottleneck: 现代微处理器能够以远高于从主存（DRAM）中读取数据的速率处理数据。 导致的结果是，很多程序受限于访存，而非计算。当然，现在也有很多访存友好的算法， 例如 BLAS 库中的 GEMM 缓存算法 缓存算法主要解决以下问题： 我们需要从主存装载哪些数据，储存在何处 缓存已满时，我们需要移出哪些数据 缓存算法的目的在于优化其 命中率（cache hit）。算法遵循以下两条原则： 空间局部性：许多算法会从连续的内存位置访问数据，有较高的空间局部性。例如如下程序： 123for (int i = 0; i &lt; size; i += 1) { max_value = max(a[i], max_value);} 起始缓存为空，访问 a[0] 时缓存未命中，需要载入数据，缓存一般一次载入一个完整的 cache line。 假设 cache line 大小是64B，数组的值是双精度浮点数（float64），则连续的8个值 a[0:8] 会一起被载入缓存， a[1:8] 的数据全部缓存命中。 时间局部性：缓存被组织为一定数目的块，即 cache line。每个块有固定的大小。缓存映射策略可以决定主存的一个特定条目的备份在缓存中的存储位置。 直接映射缓存 direct-mapping cache：主存每个特定条目在缓存中有唯一的存储位置。命中率较低。 2路组相联缓存 two-way set associative cache：从主存载入的数据可以存储在2个可能的块中，具体存储的位置由 最近最少使用（least-recently used, LRU） 原则决定，往往会选择最近时间最少使用的那个块用来存储主存载入的数据。命中率高于直接映射。常用的还有4路、8路等。 缓存一致性 假设需要修改缓存中的值，则不仅需要修改缓存中的值，还需要修改主存中的值，不然会产生不一致（inconsistency）， 有两种策略去保证缓存和主存中的一致性（coherence）： 直写式：如果主存中的数据已经缓存，则主存数据发生变动的同时也要修改缓存的值。缺点每次写主存需要一次主存访问 回写式：缓存的值修改时，不会立马修改主存的值，而是会被标记为 dirty，待数据移出缓存时，才写入主存 多级缓存和多核处理器的情况会非常复杂，例如每个处理器有自己的本地缓存L1，同时所有的处理器又共享一个公共缓存L2，每个处理器修改L1时，如果没有约束条件，可能会导致其它处理器缓存的值与修改后不一致，L1的值与L2的值也不一样。 一种方式是对于缓存且被修改的值，让其它处理器标记该数据的缓存行为失效，除非重新从主存中载入数据。常用的协议有MESI协议。 虚假共享：缓存一致性协议是对于 cache line 而言的，每一行能存多个值，如果修改了某个值，其所在的 cache line 以及其所关联的 cache line （其它核心的 cache line）将会整体失效。一个极端情况是，多个处理器同时修改一个缓存行的不同数据，任意一个写操作都会使缓存行失效，所有处理器都需要从共享主存重新载入数据，即使数据并没有改变。这种情况就是 虚假共享。 对于程序员的准则： 避免对存储在同一个缓存行中的条目进行过度更新 尽量在寄存器而不是在缓存中存储中间结果 如果数据没有被缓存，则数据仍然需要从主存中缓慢地传送出来，其访问时间在CPU或者GPU上会高达几百个时钟周期，解决的方式有 并发多线程技术 simultaneous multi-threading, SMT：在多个线程之间快速切换，用计算时间掩盖访问时间 硬件预取技术 hardware prefetching：在能预先知道处理器需要的数据时，在使用这些数据前通过硬件（目前也可由程序员自行编程实现）提前将其装载在缓存中 Flynn分类法 将处理器的并行性根据体系结构的指令和数据流分为4种类型： Flynn Taxonomy SISD：即传统的 von Neumann 体系结构，单个串行的处理单元操作单个数据 SIMD：在多个数据上并发地执行同一个操作 MIMD：多个处理单元在多个数据上进行不同的指令 MISD：多个处理单元在单个数据流上执行不同的指令，并行性少见，但可用于流水线体系结构 现代CPU和GPU在不同层次上的并行： 多重核心：集成一定数量的核心（或者多处理器构成的流水线），能异步独立执行多个线程，现代微处理器使用MIMD并行 向量单元：基于每个处理器上的SIMD向量单元实现数据并行性，例如512位的向量单元可以并行执行16对单精度浮点数的加法操作 指令级并行：通过指令流水线和超标量执行实现 SIMD：在每个时钟周期，通过向全体可用的处理单元（Processint Element, PE）或者计算逻辑单元（Arithetic Logical Unit, ALU）分发相同的指令，SIMD体系结构实现数据并行（data parallelism）机制。因此该体系结构只需要一个单独的控制单元。例如对于简单的向量逐元素相减： 123for (int i = 0; i &lt; n; i += 1) { w[i] = u[i] - v[i];} 由于循环中的每个迭代都是独立且规则的，考虑n个ALU在单独控制单元下运行，只需要将向量的每个元素u[i], v[i]放入对应的寄存器U, V，再执行简单的减法操作U-V，ALU[i]便能计算出对应的值w[i]。 但并非所有算法都对SIMD友好，例如在循环中加入条件语句： 1234567for (int i = 0; i &lt; n; i += 1) { if (u[i] &gt; 0) { w[i] = u[i] - v[i]; } else { w[i] = u[i] + v[i]; }} 为了将带有条件语句的for语句映射到SIMD体系结构上，需要允许ALU存在idle状态，即跳过一次计算，此时上述代码可以分为三步进行数据并行： 判断向量u各个元素与0的大小并标记对应的ALU是否执行第1个条件语句块，是则非idle状态，否则为idle状态 非idle状态的ALU执行语句 反转各ALU的idle状态，执行语句 在x86架构上已经可以使用SSE（流式SIMD扩展）进行向量计算。通过AVX（Advanced Vector Extension），可以使用内联函数（intrinsic）开发向量寄存器。可以实现下面程序，测试普通矩阵乘法、转置矩阵乘法和AVX转置矩阵乘法之间的性能差距： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134#include &lt;chrono&gt;#include &lt;corecrt_math.h&gt;#include &lt;cstdint&gt;#include &lt;intrin.h&gt;#include &lt;iostream&gt;#include &lt;stdint.h&gt;#include &lt;stdlib.h&gt;#include &lt;vector&gt;// #include \"include/hpc_helpers.hpp\"#define TIMERSTART(label) \\ std::chrono::time_point&lt;std::chrono::system_clock&gt; a##label, b##label; \\ a##label = std::chrono::system_clock::now();#define TIMERSTOP(label) \\ b##label = std::chrono::system_clock::now(); \\ std::chrono::duration&lt;double&gt; delta##label = b##label - a##label; \\ std::cout &lt;&lt; \"# elapsed time (\" &lt;&lt; #label &lt;&lt; \"): \" &lt;&lt; delta##label.count() \\ &lt;&lt; \"s\" &lt;&lt; std::endl;#define PRINT_MATRIX(mat, row, col) \\ for (int i = 0; i &lt; row; i += 1) { \\ for (int j = 0; j &lt; col; j += 1) { \\ std::cout &lt;&lt; mat[i * col + j] &lt;&lt; \" \"; \\ } \\ std::cout &lt;&lt; std::endl; \\ }// If the matrices are too huge, don't use this functionvoid SISDNaiveMatrixMult(std::vector&lt;float&gt; a, std::vector&lt;float&gt; b, std::vector&lt;float&gt; &amp;c, const uint64_t row_a, const uint64_t col_a, const uint64_t row_b, const uint64_t col_b) { if (col_a != row_b) { std::cout &lt;&lt; \"Shape error, two matrices can't be multiplied.\" &lt;&lt; std::endl; return; } TIMERSTART(SISD_naive_mm) for (uint64_t i = 0; i &lt; row_a; i += 1) { for (uint64_t j = 0; j &lt; col_b; j += 1) { float accum = 0; for (uint64_t k = 0; k &lt; col_a; k += 1) { accum += a[i * col_a + k] * b[k * col_b + j]; } c[i * col_b + j] = accum; } } TIMERSTOP(SISD_naive_mm)}// Transpose matrix b firstly, and then do multiplyvoid SISDTransposeMatrixMult(std::vector&lt;float&gt; a, std::vector&lt;float&gt; b, std::vector&lt;float&gt; &amp;c, const uint64_t row_a, const uint64_t col_a, const uint64_t row_b, const uint64_t col_b) { if (col_a != row_b) { std::cout &lt;&lt; \"Shape error, two matrices can't be multiplied.\" &lt;&lt; std::endl; return; } std::vector&lt;float&gt; bt(b); TIMERSTART(SISD_transpose_mm) TIMERSTART(SISD_transpose) for (uint64_t i = 0; i &lt; row_b; i += 1) { for (uint64_t j = 0; j &lt; col_b; j += 1) { bt[j * row_b + i] = b[i * col_b + j]; } } TIMERSTOP(SISD_transpose) TIMERSTART(SISD_mm) for (int i = 0; i &lt; row_a; i += 1) { for (int j = 0; j &lt; col_b; j += 1) { float accum = 0; for (int k = 0; k &lt; col_a; k += 1) { accum += a[i * col_a + k] * bt[j * row_b + k]; } c[i * col_b + j] = accum; } } TIMERSTOP(SISD_mm) TIMERSTOP(SISD_transpose_mm)}void SIMDTransposeMatrixMult(std::vector&lt;float&gt; a, std::vector&lt;float&gt; b, std::vector&lt;float&gt; &amp;c, const uint64_t row_a, const uint64_t col_a, const uint64_t row_b, const uint64_t col_b) { if (col_a != row_b) { std::cout &lt;&lt; \"Shape error, two matrices can't be multiplied.\" &lt;&lt; std::endl; return; } std::vector&lt;float&gt; bt(b); TIMERSTART(SIMD_transpose_mm) TIMERSTART(SIMD_transpose) for (uint64_t i = 0; i &lt; row_b; i += 1) { for (uint64_t j = 0; j &lt; col_b; j += 1) { bt[j * row_b + i] = b[i * col_b + j]; } } TIMERSTOP(SIMD_transpose) TIMERSTART(SIMD_mm) for (uint64_t i = 0; i &lt; row_a; i += 1) { for (uint64_t j = 0; j &lt; col_b; j += 1) { auto accum = _mm_setzero_ps(); // 128 bit, 4 float32 for (uint64_t k = 0; k &lt; col_a; k += 8) { auto ai = _mm_loadu_ps(&amp;a[i * col_a + k]); auto bi = _mm_loadu_ps(&amp;bt[j * row_b + k]); accum = _mm_add_ps(_mm_mul_ps(ai, bi), accum); auto ai2 = _mm_loadu_ps(&amp;a[i * col_a + k + 4]); auto bi2 = _mm_loadu_ps(&amp;bt[j * row_b + k + 4]); accum = _mm_add_ps(_mm_mul_ps(ai2, bi2), accum); } c[i * row_a + j] = accum[0] + accum[1] + accum[2] + accum[3]; } } TIMERSTOP(SIMD_mm) TIMERSTOP(SIMD_transpose_mm)}int main() { const uint64_t row_a = 1 &lt;&lt; 11; const uint64_t col_a = 1 &lt;&lt; 11; const uint64_t row_b = 1 &lt;&lt; 11; const uint64_t col_b = 1 &lt;&lt; 11; std::vector&lt;float&gt; a(row_a * col_a, 1.); std::vector&lt;float&gt; b(row_b * col_b, 1.); std::vector&lt;float&gt; c(row_a * col_b, 0.); // SISDNaiveMatrixMult(a, b, c, row_a, col_a, row_b, col_b); // SISDTransposeMatrixMult(a, b, c, row_a, col_a, row_b, col_b); SIMDTransposeMatrixMult(a, b, c, row_a, col_a, row_b, col_b); return 0;} 上述程序的输出为（i7-11800H）： 1234567# elapsed time (SISD_naive_mm): 174.542s# elapsed time (SISD_transpose): 0.0413579s# elapsed time (SISD_mm): 44.8414s# elapsed time (SISD_transpose_mm): 44.8848s# elapsed time (SIMD_transpose): 0.0398632s# elapsed time (SIMD_mm): 15.4761s# elapsed time (SIMD_transpose_mm): 15.5185s","categories":[{"name":"HPC","slug":"HPC","permalink":"https://aimecreative.github.io/categories/HPC/"}],"tags":[{"name":"HPC","slug":"HPC","permalink":"https://aimecreative.github.io/tags/HPC/"}],"author":"Andrew-Rey"},{"title":"HPC | Theory Backgroud","slug":"HPC/theory","date":"2024-04-11T04:38:19.000Z","updated":"2024-04-23T13:01:42.880Z","comments":true,"path":"2024/04/11/HPC/theory/","link":"","permalink":"https://aimecreative.github.io/2024/04/11/HPC/theory/","excerpt":"Parallel Programming: Concepts and Pracitce - Chapter 2","text":"Parallel Programming: Concepts and Pracitce - Chapter 2 首先介绍并行随机访问机器（PRAM）模型是抽象的共享内存模型，其忽略了现实计算机中的开销，但可以帮助设计一些并行算法。其次是对于分布式内存模型，会介绍一些基础图论知识。接着介绍并行程序中的两大定律：Amdahl定律和Gustafson定律，用于推断并行程序加速比能达到的上限。最后以并行算法设计的Foster方法论结束。 并行随机访问机器模型 事实上，PRAM (Parallel Random Access Machine) 模型架构十分简单，相比于操作系统课上的一个处理器而言，PRAM拥有多个独立的处理器，每个处理器分3个阶段执行一个指令周期： 读阶段：每个处理器并发地从各自的共享内存中读取单条数据并保存到本地的寄存器中。 计算阶段：每个处理器对本地数据执行一个基本操作，并将结果存储在寄存器中。 写阶段：每个处理器并发写一条数据到共享内存中。 PRAM中的通信通过处理器在共享内存中的读写实现，该类型的内存能够以统一的方式访问，即每个处理器对内存中任意位置的访问都使用统一的常数时间实现，这和现实计算机很不一样（访问大规模共享内存时耗费的时间不一致）。 PRAM的变体 在相同的指令中期中，多个处理器读写多个共享内存单元会发生冲突，为解决冲突，出现了如下的几种PRAM变体：ER (exclusive read)，EW (exclusive write)，CR (concurrent read)，CW (concurrent write)。常见的组合有三种： EREW：独占读、独占写。任意周期内，不允许多个处理器在相同的共享内存单元中进行读写。 CREW：并发读、独占写。 CRCW：并发读、并发写。对于并发写入，有常见的数据保留形式： Priority: 处理器本身的优先级决定 Arbitrary: 随机选取一个处理器的值写入 Common: 若所有的值都相等则写入，否则内存位置的值不变 Combining: 通过某种运算组合所有的冲突值再写入 PRAM上的前缀和算法 问题描述：给定 个数据，和一个该数据的二元运算符，假设为加法运算。在一台拥有 个计算结点的PRAM上，并行计算前缀和。 其中数据已经存储在了共享内存 A 中，每个计算结点的寄存器用 reg 表示，目标是设计一个开销最优化的PRAM算法。 串行分析：使用一个计算结点求解前缀和问题 1for (int i = 1; i &lt; n; i += 1) A[i] += A[i-1]; 计算复杂度为 。 并行分析：使用 个计算结点并行求解前缀和问题 当 时，计算结点的数量和数据量相等，每个计算结点上处理一个数据。 可以使用分治递归的方式，将计算结点逐一合并。 1234567891011121314151617//-- 算法 1 --//// load data for every node@parallelfor (int i = 0; i &lt; p; i += 1) { reg[i] = A[i];}// total iteration num, merge by 2for (int i = 0; i &lt; ceil(log(p)); i += 1) { // the left nodes have been calculated int node_start_idx = pow(2, i); @parallel for (int j = node_start_idx; j &lt; p; j += 1) { reg[j] += A[j - node_start_idx]; A[j] = reg[j]; }} 总体的计算结构类似于二叉树：每一个结点都与左边相邻结点进行计算前缀和，第 次迭代中，每 个结点视为一个 merge 的结点， 因此一共需要 次递归，即花费的时间为 ， 开销为 ，是对数线性的。 如果需要继续减小开销 ，则要么减小 ，要么减小 ，降低运行时间比较困难，因此选择减少计算结点的数量 ，即 方法如下 我们有 个计算结点，先将 个数据均分到每个计算结点上，每个结点有 个数据 每个计算结点对本地内存的数据求解前缀和，花费的时间为 每个结点返回本地前缀和的最后一位结果，得到一共 个数据 对上述 个数据执行算法1，花费的时间为 ，计算完成后依然得到长度为 的前缀和 A_p 将第4步得到的前缀和 A_p[j]，依次加到 reg[j+1] 上，A_p 的最后一位不用加，由于每个结点有 个数据，因此花费的时间为 综上所述，整个算法的时间为 ，开销为 ， 当 时，计算时间为对数，且开销为线性的。 123456789101112131415161718192021222324252627282930313233//-- 算法 2 --//// stage 1-3// calculate prefix sum for every node// each node contains k = n/p = log(n) datak = n/p = log(n)@parallelfor (int i = 0; i &lt; p; i += 1) { for (int j = 1; j &lt; k; j += 1) { // data index: i * num + offsets A[i*k+j] += A[i*k+j-1] }}// stage 4// calculate prefix sum for rightmost values of every nodefor (int i = 0; i &lt; log(p); i += 1) { int node_start_idx = pow(2, i); @parallel for (int j = node_start_idx + 1; j &lt; p; j += 1) { A[j*k-1] += A[(j - node_start_idx)*k-1] }}// stage 5// add results@parallelfor (int i = 1; i &lt; p; i += 1) { // ignore the last value for (int j = 0; j &lt; k-1; j += 1) { A[i*k+j] += A[i*k-1] }} PRAM上的稀疏矩阵压缩算法 稀疏矩阵压缩算法可以利用前缀和算法。 问题描述：稀疏数组 A 中多个元素为 ，希望能通过并行算法压缩为非零数组 V 和对应的位置数组 C。 构造和 A 等长的临时数组 temp，其中若 temp[i] = 1 if A[i]!=0 else temp[i]=0， 将数组 A 和临时数组 temp 均分到 个计算结点上，并行生成临时数组和计算临时数组的前缀和 求完前缀和的临时数组目前可以作为稀疏数组的 地址列表，接下来根据临时数组，并行索引 A 中对应地址，得到非零值和位置，写入 V,C 即可 分析： 网络拓扑 互联网络的结点可能是交换机或处理器。几个概念： 度(degree)：网络的度表示所有结点中邻居数目的最大值 对分宽度(bw)：将网络分为二分图，两个分图间边的最小值 直径(diam)：任意两个结点之间全部最短路径的最大值 在设计互联网络时，经常关注以下 理想属性： 常数度：网络的度是常数，即与网络的规模无关。这个属性允许网络扩大到更大的规模而无需增加过多的连接数 小直径：可以支持任意进程之间的高效通信 高对分宽度：对分宽度越低，大量聚合的通信操作会变得更慢，它隐含的是网络的内部带宽 经典网络拓扑结构各属性的阶： topology degree diam bw 线性排列 2D网面/环面 3D网面/环面 二叉树 超立方体 Amdahl's Law and Gustafson's Law 如果能在设计并行算法前，能对一个问题提前分析其并行效果，将会减少不必要的工作量，也能了解到并行算法是否值得， 而 Amdahl 定律和 Gustafson 定律能帮助我们进行加速比的估计。 一段程序的总执行时间可以分为未被并行化部分所花费的时间（即不能被并行化或没有被并行化的部分所花的时间） 和并行运行的时间，分别用符号 表示。 单个处理器运行一段程序的时间 即上述两部分的简单相加： Amdahl's Law 在不考虑缓存效应的前提下，我们希望最佳加速比是线性的： 也就是说如果有 个处理器并行一段程序，其并行部分能比单个处理器运行快 倍， 因而可以导出一般情况下的运行时间下限： 因此可以得到其加速比的上限： 一般而言，在只讨论串行时间和并行时间时，我们使用百分比来表示二者的关系： 其中 是一个介于0，1之间的数，此时的加速比可以表示成 的函数： 这便是 Amdahl 定律。通过知道 ，我们就能预测使用多个处理器并行化程序的加速比理论上限。 Amdahl定律的限制：只适用于问题规模为 常数、处理器个数变化的情况，即强可扩展性。 Gustafson's Law 如果在增加处理器个数的情况下，同时增大问题规模，花在并行部分的时间 比串行时间 增长得更快。 为了同时考虑这些情况，可以按照问题的复杂性扩展两个部分的规模： ：根据问题规模的复杂度，不能从并行化中获益的程序部分的 尺度函数 ：根据问题规模的复杂度，能从并行化中获益的程序部分的尺度函数 对于单个处理器的情况，程序运行时间为： 因此得到可达加速比（即并行的最高加速比按照线性加速比处理）： 令 ： 当 时，即问题规模增加时，串行和并行的尺度增加一致，此时为Amdahl定律； （Gustafson 定律）当 时，，即可并行部分以线性 增长，不可并行部分保持常数 Foster的并行算法设计方法学","categories":[{"name":"HPC","slug":"HPC","permalink":"https://aimecreative.github.io/categories/HPC/"}],"tags":[{"name":"HPC","slug":"HPC","permalink":"https://aimecreative.github.io/tags/HPC/"}],"author":"Andrew-Rey"},{"title":"HPC | Introduction to Parallel Computation","slug":"HPC/intro","date":"2024-04-08T16:04:32.000Z","updated":"2024-04-11T14:56:50.579Z","comments":true,"path":"2024/04/09/HPC/intro/","link":"","permalink":"https://aimecreative.github.io/2024/04/09/HPC/intro/","excerpt":"Parallel Programming: Concepts and Pracitce - Chapter 1","text":"Parallel Programming: Concepts and Pracitce - Chapter 1 概念 加速比 Speedup：衡量一个并行算法比串行算法快多少的指标。即使用单个处理器运行程序所花费的时间 与使用 个处理器运行程序所花费的时间 之比 通常我们希望得到的加速比为线性加速比，即用 个处理器去运行程序，最大的加速比为 效率 Efficiency：定义为加速比和处理器数目之比，衡量了平均一个处理器带来的加速比。当效率为 时，此时为线性加速比 可扩展性 Scalability：分为强可扩展性和弱可扩展性。 强可扩展性 Strong Scalability：测量效率时仅改变处理器的数目，输入数据的规模保持不变 弱可扩展性 Weak Scalability：处理器的数目随着输入数据规模共同变化（处理器数目翻倍时，测量效率时把数据规模也翻倍） 计算通信比 Computation-to-communication Ratio：定义为计算花费的时间和处理器间处理消息通信花费的时间之比。 分布式内存系统：每个计算单元只能访问自己的本地内存，如果需要访问其它单元，需要通过一个显式的通信步骤（例如通信网络）实现。 共享式内存系统：所有计算单元共享内存，除此之外，自己本身也有更小的内存（分级缓存）。 并行程序设计时需要考虑划分（数据并行、任务并行、模型并行）、通信、同步和负载平衡等。 求和的例子 现在我们进行一组数据的加法求和操作，其中数据量为 ，处理器数量为 。设 为一次加法操作所需要的时长， 为一批数据的通信时长。则 数据分发次数： 每个处理器本地求和： 每个处理器将结果传递给一个处理器（数据收集）： 中间结果求和： 总的求和运行时长为 其加速比为 对于固定的 ，加速比只与计算通信比 有关，并且有 因此在固定数据规模和处理器数量时，要提高加速比，需要降低计算通信比。同时，加速比也可以是处理器数量的函数： 令偏导为 ，解出最值条件 综上所述，有如下规律： 当数据规模固定时，加速比依赖于采用的计算单元的数目和计算通信比 通常情况下，加速比随着计算单元的增加达到局部最大，但使用更多计算单元时，加速比会降低 最优的加速比依赖于计算通信比，通信时长占比越大，使用的计算单元数目应该越少 前缀和的例子 前缀和问题：现有 个数据和 个计算结点 输入：一个二元可结合运算符 ； 个待运算的数据 输出： 个数据 ，其中对于 由于计算 需要依赖 ，在循环分析时带来了一定的困难，但该问题依然有并行的方式。 数据划分：使用 分治 策略，将 个数据按顺序均分为 份，分别分给 个计算结点，每个结点计算本地内存的前缀和，花费的时间为 数据归并：递归 合并相邻的结点数据，即左边结点将前缀和的 最后一个结果 返回，传递给右边结点，右边结点接收左边的前缀和，并将其与自己结点结果求和 所有线程将结果返回，计算完成 perfix sum 前缀和问题将在后续进行更加详细的讨论。","categories":[{"name":"HPC","slug":"HPC","permalink":"https://aimecreative.github.io/categories/HPC/"}],"tags":[{"name":"HPC","slug":"HPC","permalink":"https://aimecreative.github.io/tags/HPC/"}],"author":"Andrew-Rey"}],"categories":[{"name":"Lecture","slug":"Lecture","permalink":"https://aimecreative.github.io/categories/Lecture/"},{"name":"Multi-Modal","slug":"Multi-Modal","permalink":"https://aimecreative.github.io/categories/Multi-Modal/"},{"name":"Compiler","slug":"Compiler","permalink":"https://aimecreative.github.io/categories/Compiler/"},{"name":"You","slug":"You","permalink":"https://aimecreative.github.io/categories/You/"},{"name":"HPC","slug":"HPC","permalink":"https://aimecreative.github.io/categories/HPC/"}],"tags":[{"name":"Lecture","slug":"Lecture","permalink":"https://aimecreative.github.io/tags/Lecture/"},{"name":"Database","slug":"Database","permalink":"https://aimecreative.github.io/tags/Database/"},{"name":"Multi-Modal","slug":"Multi-Modal","permalink":"https://aimecreative.github.io/tags/Multi-Modal/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://aimecreative.github.io/tags/Algorithm/"},{"name":"Compiler","slug":"Compiler","permalink":"https://aimecreative.github.io/tags/Compiler/"},{"name":"You","slug":"You","permalink":"https://aimecreative.github.io/tags/You/"},{"name":"HPC","slug":"HPC","permalink":"https://aimecreative.github.io/tags/HPC/"}]}